{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMCyWmGjFjoU"
      },
      "source": [
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003eCopyright 2021 DeepMind Technologies Limited.\u003c/p\u003e\n",
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003e Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at \u003c/p\u003e\n",
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003e \u003ca href=\"https://www.apache.org/licenses/LICENSE-2.0\"\u003ehttps://www.apache.org/licenses/LICENSE-2.0\u003c/a\u003e \u003c/p\u003e\n",
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003e Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. \u003c/p\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fq4w5ZKGATx"
      },
      "source": [
        "# **Unsupervised learning** \n",
        "\u003ca href=\"https://colab.research.google.com/github/deepmind/educational/blob/master/colabs/summer_schools/intro_to_unsupervised_learning.ipynb\" target=\"_parent\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e\n",
        "\n",
        "\n",
        "**Aim**\n",
        "Provide you with the basics of the unsupervised learning. It is intended as a practical guide, so do not expect a solid theoretical background. You'll learn about the connection between neural networks and probability theory, how to build and train an autoencoder with only basic python knowledge, and how to compress an image using the $\\mathrm{K\\!-\\!means}$ clustering algorithm. Lots of visualisations and exercises are included to make this journey fun.\n",
        "\n",
        "**Disclaimer**\n",
        "\n",
        "This code is intended for educational purposes, and in the name of readability for a non-technical audience does not always follow best practices for software engineering.\n",
        "\n",
        "**Links to resources**\n",
        "- [What is Colab?](https://colab.sandbox.google.com/notebooks/intro.ipynb) If you have never used Colab before, get started here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6iHUYY9H5_P"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "Machine learning is often categorised into three types: \n",
        "- **Supervised learning**, which provides the machine with input-output pairs, i.e. for each observation, the user defines the desired output which the machine needs to learn;\n",
        "- **Reinforcement learning**, where instead of target outputs, the machine receives a more general feedback (the reward), which it tries to maximise (e.g. winning at chess);\n",
        "- **Unsupervised learning**, which works solely with the observations.\n",
        "The machine is expected to discover patterns in the data and create their compact representation. \n",
        "\n",
        "Typical topics addressed in unsupervised learning are:\n",
        "- **Density estimation**, which models the probability distribution of data, $p(x)$;\n",
        "\u003cimage src=\"https://storage.googleapis.com/dm-educational/assets/unsupervised-learning/density_estimation.png\" \u003e\n",
        "- **Clustering**, which seeks to discover (and characterise) groups of similar examples in the data (data clusters);\n",
        "\u003cimage src=\"https://storage.googleapis.com/dm-educational/assets/unsupervised-learning/clustering.png\" \u003e\n",
        "- **Dimensionality reduction / blind source separation / latent variable models**, which enable analysis and visualisation, as well as compression by extracting the most informative pieces of information from a dataset (e.g. Principle Component Analysis, Factor Analysis).\n",
        "\n",
        "These goals are interrelated. For example, we will see how clustering can be cast as a way to estimate data density and how it can enable data compression. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxWiIiBV-i2K"
      },
      "source": [
        "**Probabilistic Models** and **Neural Networks** are two main families of Unsupervised Learning algorithms. There are many correspondences between the two and thus knowing the basics of probability theory helps in developing a deeper understanding of Unsupervised Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yFQd6j4kJOG"
      },
      "source": [
        "**Famous Neural Networks for Unsupervised Learning**\n",
        "\n",
        "One of the most famous networks for clustering is an Autoencoder (AE). If you have never come across the name, [this blog post](https://hackernoon.com/how-to-autoencode-your-pok%C3%A9mon-6b0f5c7b7d97) offers a fun introduction. \n",
        "\n",
        "Neural networks for density estimation are called normalizing flows, they output $p(x)$ directly. Example neural networks for clustering are Self-Organising Maps (SOM) and Neural Gas. Probably the most famous latent variable model is the Variational Autoencoder (VAE). Similar to the classical autoencoder, VAE computes a compressed representation of the data - the latent code. However, unlike AE, VAE is in a position to produce new data samples, i.e. it is a generative model. Another famous generative model is Generative Adversarial Network (GAN), although it is less motivated by probabilistic modelling (and difficult to evaluate). \n",
        "\n",
        "For more on neural networks in unsupervised learning, check out this great [INDABA tutorial](\n",
        "https://github.com/deep-learning-indaba/indaba-pracs-2019/blob/master/3b_generative_models.ipynb)\n",
        "which covers both VAEs and GANs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9yPpmG0Tq1v"
      },
      "source": [
        "**In this tutorial**, we cover the basics of [density estimation](#scrollTo=HU-jyOW-HTlQ\u0026line=2\u0026uniqifier=1), [clustering](#scrollTo=JuzhThGFPJ_M\u0026line=4\u0026uniqifier=1) and [latent variable modelling](#scrollTo=mT5bu54-pXnj). We also cover the neural network basics by coding up a simple [autoencoder](#scrollTo=nits21YIkI_h). We finish with an example of how clustering can be used for [image compression](#scrollTo=m7LBpsTSIhk_).\n",
        "\n",
        "\n",
        "**Further reading:** [Chris Bishop. Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/). \n",
        "Many exercises in this tutorial were inspired by this excellent book.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn9g-zUqJU6Y"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQs-QcP_SwDL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Basic plotting in style:\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Animations:\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V9KoNPwIdLJG"
      },
      "outputs": [],
      "source": [
        "#@title Data generating functions\n",
        "# 0, 2, 5, 10, 80\n",
        "#https://tall.life/height-percentile-calculator-age-country/\n",
        "HIDDEN_MU = [49.3, 50, 86.2, 87.6, 108.1, 109.4, 139.1, 138, 155.9, 170.7]\n",
        "HIDDEN_SIGMA = [1.88, 1.87, 3.24, 3.08, 4.91, 4.61, 6.97, 6.88,  6.32, 6.63] \n",
        "def get_data_a(n=1000, k=0):\n",
        "  return np.random.randn(n) * HIDDEN_SIGMA[k] + HIDDEN_MU[k]\n",
        "\n",
        "HIDDEN_PI = np.random.dirichlet([1, 1, 1])\n",
        "#https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/bulletins/annualmidyearpopulationestimates/mid2019estimates\n",
        "HIDDEN_PI = np.array([.514, .486, .513, .487, .512, .488, .514, .486, .55, .45])\n",
        "HIDDEN_PI /= np.sum(HIDDEN_PI) \n",
        "def get_responsibilities(p, n=10000):\n",
        "  n_per_cluster = np.random.multinomial(n, p)\n",
        "  r = [[k] * n_k for k, n_k in enumerate(n_per_cluster)]\n",
        "  r = np.concatenate(r)\n",
        "  np.random.shuffle(r)\n",
        "  return r\n",
        "\n",
        "R = get_responsibilities(HIDDEN_PI)\n",
        "def get_data_b():\n",
        "  y = np.random.randn(len(R))\n",
        "  for t, k in enumerate(R):\n",
        "    y[t] *= HIDDEN_SIGMA[k]\n",
        "    y[t] += HIDDEN_MU[k]\n",
        "  return y\n",
        "\n",
        "def get_data_c(n_x=6):\n",
        "  return np.eye(n_x * 3).reshape((-1, n_x, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3pE22jNWduis"
      },
      "outputs": [],
      "source": [
        "#@title Plotting settings\n",
        "plt.rcParams['figure.figsize'] = [12, 2.5]\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_context('notebook', font_scale=1.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU-jyOW-HTlQ"
      },
      "source": [
        "# **Density estimation**\n",
        "Density estimation aims to describe observed data by approximating its probability density function.\n",
        "\n",
        "In practice, datasets are finite, and thus we often consider probability mass rather than densities. For continous variables, this requires a binning process, which combines the number of possible values into discrete sets (bins). \n",
        "\n",
        "The data itself forms an **empirical data distribution**: it reflects how many samples of a given kind were observed. An alternative estimate is the **parametric data distribution**, which refers to a set of parameters, $\\Theta$, such as the mean and variance of a Gaussian. Parametric data distributions allow for an approximate, concise description of the data. They can also relate the underlying data generating process.\n",
        "\n",
        "In this section, we will consider a continuous random variable. We will \n",
        "- visualise the binning process with a histogram, \n",
        "- normalise the binned counts to obtain the empirical data distribution and density estimate, and also \n",
        "- approximate it with a parametric data distribution (a Gaussian).\n",
        "\n",
        "But how do we find the best parameters to fit the data? \n",
        "And what do we even mean by \"the best\"? We cover these important topics in the remainder of this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E6VOvWwL6ku"
      },
      "source": [
        "Let us look at the first dataset: the height of newborn baby girls. (Samples were generated according to statistics available [online](https://tall.life/height-percentile-calculator-age-country/)). \n",
        "\n",
        "Height is a continuous variable, so we start by binning the data. This process is best visualised with a **histogram**. A histogram divides the domain into discrete, nonoverlapping sets (bins) and it measures how many samples fall within each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYl2fGU6dmx-"
      },
      "outputs": [],
      "source": [
        "x_a = get_data_a()\n",
        "counts, bins, _ = plt.hist(x_a, bins=20)\n",
        "plt.title(f'Histogram of our data (N={len(x_a)})')\n",
        "plt.ylabel('Counts (bin)')\n",
        "plt.xlabel('x (cm)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubcTfyRzgMyU"
      },
      "source": [
        "By default, `plt.hist()` function displays the number of counts per bin, $n_k$. For probability mass, we need to divide the bin counts by the number of all samples, $N$.\n",
        "$$P(x\\in bin_k) = \\frac{n_k}{N}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08259--K_8j2"
      },
      "outputs": [],
      "source": [
        "p_bins = counts / np.sum(counts)\n",
        "\n",
        "# Let's plot the distribution\n",
        "dx = bins[1] - bins[0]\n",
        "bin_centers = bins[:-1] + dx / 2\n",
        "plt.bar(bin_centers, p_bins, dx)\n",
        "plt.title('Empirical data distribution')\n",
        "plt.ylabel('P(x in bin)')\n",
        "plt.xlabel('x (cm)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT3D9vDXE9d0"
      },
      "source": [
        "Thus, we arrived at an empirical probability distribution, $P(x\\in bin_k)$ over a discrete number of bins. \n",
        "\n",
        "To treat the continuous $x$ directly, we have to estimate probability density, $p(x)$. \n",
        "\n",
        "One straightforward way to do this is to divide the probability mass by bin size, $|bin_k|$. By default `plt.hist()` uses bins of equal length, $\\Delta x$, which simplifies the procedure: \n",
        "$$p(x\\in bin_k) = \\frac{P(x\\in bin_k)}{|bin_k|} = \\frac{P(x\\in bin_k)}{\\Delta x}$$\n",
        "This is what we get when setting `plt.hist(x, density=True)`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCuRt4DaH-b1"
      },
      "outputs": [],
      "source": [
        "p_bins_density = p_bins / dx\n",
        "plt.step(bin_centers, p_bins_density, \n",
        "         where='mid', c='tab:orange', lw=3, label=r'P(bin)/|bin|')\n",
        "plt.hist(x_a, bins=20, density=True, label='hist(x, density=True)')\n",
        "\n",
        "plt.title('Empirical data density')\n",
        "plt.ylabel('p(x)')\n",
        "plt.xlabel('x (cm)');\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoV1hyCKXk67"
      },
      "source": [
        "Do you recognise this shape? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCbmRb-t__oX"
      },
      "source": [
        "##**Gaussian distribution**\n",
        "\n",
        "The **normal distribution** (**Gaussian distribution**) is one of the most important distributions describing real-valued observations, $x\\in\\mathbb{R}$. It is characterised by two parameters, the mean ($\\mu\\in\\mathbb{R}$) and the variance ($\\sigma^2 \\in\\mathbb{R_+}$). \n",
        "$$p(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \n",
        " e^{\\left . -(x - \\mu)^2\\small \\right / 2\\sigma^2} $$\n",
        "\n",
        "\n",
        "The constant term ensures that, as with all probability distributions:\n",
        "$$\\int_{-\\infty}^\\infty p(x|\\mu, \\sigma^2) dx = 1.$$\n",
        "(For a discrete random variable, we would use a sum rather than an integral over all the possible values of $x$.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk4uM5sW0GA6"
      },
      "source": [
        "Let's plot the Gaussian probability density. The shape of this curve is often referred to as the \"bell curve\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18XNIOYJtBIs"
      },
      "outputs": [],
      "source": [
        "def bell_curve(x, mu, sigma_2):\n",
        "  y = np.exp(- (x - mu)**2 / 2 / sigma_2)\n",
        "  y /= np.sqrt(2 * np.pi * sigma_2)\n",
        "  return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoiDwo3R_9Og"
      },
      "source": [
        "Let's use the empirical mean and variance as the parameters of our Gaussian:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuBOOrPk4m2j"
      },
      "outputs": [],
      "source": [
        "x_mean, x_var, n_x = np.mean(x_a), np.var(x_a), len(x_a)\n",
        "print('Mean of our data: {:.2g}cm, variance: {:.2g}cm^2 (N={})'.format(x_mean, x_var, n_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWD0fPXdigWG"
      },
      "outputs": [],
      "source": [
        "x_ = np.linspace(np.min(x_a), np.max(x_a), 100)\n",
        "p_x = bell_curve(x_, x_mean, x_var)\n",
        "plt.plot(x_, p_x)\n",
        "plt.title(r'Gaussian probability density')\n",
        "plt.ylabel('p(x)')\n",
        "plt.xlabel('x (cm)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6sO8KwK2I42"
      },
      "source": [
        "Indeed, the shape is similar to our data distribution! Let's overlay the two:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNnUVuT99eK3"
      },
      "outputs": [],
      "source": [
        "plt.hist(x_a, bins=20, density=True, label='data')\n",
        "plt.plot(x_, p_x, lw=3, label='fit')\n",
        "plt.title('Empirical data density')\n",
        "plt.ylabel('p(x)')\n",
        "plt.xlabel('x (cm)');\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M6f0i_YCUiX"
      },
      "source": [
        "Note, that we could compare probability masses $P(x\\in bin)$ rather than densities $p(x)$. We just need to integrate the density over the bins \n",
        "$$P_{\\mu, \\sigma^2}(x\\in bin) = \\int_{x \\in bin} p(x |\\mu, \\sigma^2) dx.$$ \n",
        "In practice, we will approximate it with $P_{\\mu, \\sigma^2}(x\\in bin) \\sim p(\\hat x|\\mu, \\sigma^2) \\Delta x$, with $\\hat x$ corresponding to the center of the bin. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-bp75rW-DMz"
      },
      "outputs": [],
      "source": [
        "plt.bar(bin_centers, p_bins, dx, label='data')\n",
        "\n",
        "p_gauss_bins = dx * bell_curve(bin_centers, x_mean, x_var)\n",
        "plt.step(bin_centers, p_gauss_bins, where='mid', c='tab:orange', lw=3, \n",
        "         label='fit')\n",
        "\n",
        "plt.title('Empirical data distribution and its Gaussian fit')\n",
        "plt.ylabel('P(x in bin)')\n",
        "plt.xlabel('x (cm)');\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Wo6L1MGoR9"
      },
      "source": [
        "## **Maximum Likelihood Estimates (MLE)**\n",
        "Our dataset is very well described by the normal distribution with parameters estimated from the dataset. This was for a good reason. The empirical mean and variance are obtained by maximising the likelihood of data under Gaussian probability: \n",
        "$$\\mathcal L (\\mathcal D|\\mu, \\sigma^2)= \\prod_{n=1}^N p(x_n|\\mu, \\sigma^2).$$\n",
        "\n",
        "In general, for a chosen parametric distribution $P(X|\\Theta)$ and some data $\\mathcal D$, the **likelihood** is defined as\n",
        "$$\\mathcal L (\\mathcal D|\\Theta)= \\prod_{x_n\\in\\mathcal D} p(x_n|\\Theta).$$ \n",
        "$\\mathcal L (\\mathcal D|\\Theta)$ is often used as a measure of how well $P(X|\\Theta)$ fits the data. \n",
        "\n",
        "The parameters that optimise this measure are called **maximum likelihood estimates (MLE)**,  $$\\text{MLE}={\\arg\\max}_\\Theta \\mathcal L (\\mathcal D|\\Theta).$$\n",
        "\n",
        "Likelihood is probably the most popular measure of the quality of fit in a probabilistic setting. However, it is numerically challenging: with a growing number of samples, its values quickly approach zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MymeVmvl-ura"
      },
      "source": [
        "### **Log-Likelihood**\n",
        "Instead of evaluating the likelihood, it's much more practical to evaluate its logarithm, $\\log \\left(\\mathcal L(\\mathcal D)\\right)$. Log-likelihood makes it easier to keep track of values close to zero. It also turns the massive product $\\prod_n$ into a sum:\n",
        "$$\\log\\left(\\mathcal \\prod_n p (\\mathcal x_n|\\theta)\\right) \n",
        " =\\sum_n \\log (p (\\mathcal x_n|\\theta)),$$\n",
        "vastly simplifying differentiation.\n",
        "\n",
        "Importantly, the logarithm is a monotonically increasing function, so the MLE will be the same for $\\mathcal L$ and $\\log (\\mathcal L)$, \n",
        "$${\\arg\\max}_\\Theta \\mathcal L (\\mathcal D|\\Theta) = {\\arg\\max}_\\Theta \\log(\\mathcal D|\\Theta ).$$\n",
        "\n",
        "Let's compute the log-likelihood for our Gaussian distribution:\n",
        "\\begin{align}\n",
        "\\sum_n \\log [p(x_n|\\mu, \\sigma^2)] = \\sum_n  \\left(\\frac{-(x_n - \\mu)^2 }{ 2\\sigma^2 } - \\frac 1 2 \\log 2\\pi\\sigma^2\\right)\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zQP5PRcCEe-"
      },
      "source": [
        "### **Log-Loss**\n",
        "Machine learning prefers to use the language of **cost** or **loss** functions that need to be minimised. Thus, you might come across **log-loss**, which is nothing else than a negative log-likelihood:\n",
        "$$C(\\mathcal D|\\Theta) = -  \\log(\\mathcal L (\\mathcal D|\\Theta)).$$\n",
        "\n",
        "Yet again, optimising log-loss recovers MLE:\n",
        "$${\\arg\\max}_\\Theta \\mathcal L (\\mathcal D|\\Theta) = {\\arg\\min}_\\Theta C(\\mathcal D|\\Theta)$$\n",
        "\n",
        "This is the cost function used by most neural networks in the unsupervised setting. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMiJqeTSKiFD"
      },
      "source": [
        "###**Task: Optimise by hand** \n",
        "Using the sliders, try to do the NN's job and minimise the cost function. How well can you fit our data distribution? What happens (and why) when the variance is zero?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo5TMXBfTxUJ"
      },
      "outputs": [],
      "source": [
        "def log_loss_gaussian(x_, mu, sigma_2):\n",
        "  log_p = ((x_ - mu)**2 / sigma_2  + np.log(sigma_2) + np.log(2 * np.pi)) / 2\n",
        "  return np.sum(log_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MDCKobNRCD5d"
      },
      "outputs": [],
      "source": [
        "#@title Fit a Gaussian {run: \"auto\"}\n",
        "mu = 50.1597 #@param{type:'slider', min:40, max:60, step:1e-4}\n",
        "sigma_2 = 9.8342 #@param{type:'slider', min:0, max:100, step:1e-4}\n",
        "\n",
        "plt.hist(x_a, bins=20, density=True, label='data',\n",
        "         facecolor='.9', edgecolor='.3')\n",
        "plt.plot(x_, bell_curve(x_, mu, sigma_2), lw=3, label='fit')\n",
        "plt.title('Negative log-likelihood = {:.2f}'.format(log_loss_gaussian(x_a, mu, sigma_2)))\n",
        "plt.ylabel('p(x)')\n",
        "plt.xlabel('x (cm)');\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoSEEAtUIk-l"
      },
      "source": [
        "##**Extra reading: Derivation of MLE**\n",
        "**1. Use log-likelihood** \n",
        "As discussed above, maximising log-likelihood is equivalent to maximising likelihood. For a Gaussian:\n",
        "\\begin{align}\n",
        "\\sum_n \\log [p(x_n|\\mu, \\sigma^2)] = \\sum_n  \\left(-\\frac{(x_n - \\mu)^2 }{ 2\\sigma^2 } - \\frac 1 2 \\log 2\\pi\\sigma^2\\right)\n",
        "\\end{align}\n",
        "\n",
        "**2. At maximum, the gradients should be zero** \n",
        "\n",
        "Now we need to compute the gradients of the log-likelihood with respect to the parameters $\\mu$ and $\\sigma^2$:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{d \\log \\left[\\mathcal L(\\mathcal D)\\right]}{d\\mu} \u0026= \\sum_n \\frac{2(x_n - \\mu)}{2\\sigma^2} = \\frac{1}{\\sigma^2} \\left(\\sum_n x_n - N \\mu \\right)\\\\\n",
        "\\frac{d \\log \\left[\\mathcal L(\\mathcal D)\\right]}{d\\sigma^2} \u0026= \n",
        "\\sum_n \\left(\\frac{(x_n - \\mu)^2}{2} \\frac{1}{(\\sigma^2)^2} - \\frac{1}{2 \\sigma^2} \\right)= \\frac{1}{2(\\sigma^2)^2} \\left(\\sum_n (x_n -\\mu)^2 - N \\sigma^2\\right)\n",
        "\\end{align}\n",
        "\n",
        "Setting these gradients to zero we get: \n",
        "$$\\mu_{\\mathrm{MLE}}= \\frac 1 N \\sum_n (x_n) \\qquad \\sigma^2_{\\mathrm{MLE}} = \\frac 1 N \\sum_n (x_n -\\mu)^2$$\n",
        "which you might recognise as definitions of empirical mean and variance. \n",
        "\n",
        "**3. At maximum, the second derivative should be positive**\n",
        "\n",
        "We leave it as an exercise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuzhThGFPJ_M"
      },
      "source": [
        "# **Clustering with $\\mathrm{K\\!-\\!means}$**\n",
        "Our first dataset was quite simple, it was well approximated by a Gaussian distribution. Thus, mean (49cm) and variance (3.7cm$^2$) were statistics sufficient to succinctly describe it. But in real life, that's rarely the case. \n",
        "\n",
        "In this section, we explore one of the most important algorithms for clustering: $\\mathrm{K\\!-\\!means}$. For this purpose, we load another dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN7UUrIF2FKy"
      },
      "outputs": [],
      "source": [
        "x_b = get_data_b()\n",
        "plt.hist(x_b, bins=200, density=True, histtype='stepfilled');\n",
        "plt.title('Empirical data density')\n",
        "plt.ylabel('p(x)')\n",
        "plt.xlabel('x (cm)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGugo-nGJKo0"
      },
      "source": [
        "Clearly, the bell curve is no match for this dataset - its distribution is multimodal (it has many peaks). It seems appropriate to instead describe it by clusters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss08kvMfY0Jl"
      },
      "source": [
        "##**$\\mathrm{K\\!-\\!means}$**\n",
        "\n",
        "The $\\mathrm{K\\!-\\!means}$ algorithm assumes that data can be well described by clusters. The number of clusters ($K$) needs to be decided upfront. Each cluster is associated with a template, i.e. the mean of all samples that belong to the cluster. The $\\mathrm{K\\!-\\!means}$ algorithm iteratively decides which cluster the data belongs to and what is the template most suitable for each cluster.\n",
        "\n",
        "1. Input: Data ($\\mathcal{D}\\in \\mathbb{R}^{d \\times N}$) and number of clusters $K$.\n",
        "\n",
        "  Here, we consider a one-dimensional case, $d=1$, with $N=3000$ samples. We make a guess $K=4$.\n",
        "2. Initialise the templates for each cluster $k$:  $\\mu_k\\in\\mathbb{R}^{d}$.\n",
        "3. Repeat (for a pre-defined number of steps or until convergence):\n",
        "  -  **$\\mathrm{E}$-step**: For each sample $n$, decide which cluster it belongs to, $z_n\\in\\{1, \\ldots, K\\}$.   $z_n$ is a categorical variable which minimises distance between the sample and the cluster: $z_n = \\arg \\min_k D(x_n, \\mu_k)$. \n",
        "\n",
        "    In our 1-d case, measuring distance amounts to checking the absolute value of the scalar difference:  $D(x_n, \\mu_k) = |x_n-\\mu_k|$. \n",
        "    \n",
        "  - **$\\mathrm{M}$-step**: update $\\mu_k$ by computing the mean over all samples assigned to a given cluster: $\\mu_k= \\langle x_n\\rangle_{\\mathrm z_n=k}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHIK79HTLnZ5"
      },
      "source": [
        "**Why do we call the steps E and M?**\n",
        "\n",
        "The reason we refer to the iterative steps as E and M is that $\\mathrm{K\\!-\\!means}$ algorithm can be cast as a special case of the Expectation Maximisation algorithm. \n",
        "- $\\mathrm{E}$-step computes the expectation over our latent variable $z_n$ for each data sample $n$ (only one cluster can be assigned to any data sample, so our \"expectation\" is simply the id of the most likely cluster). \n",
        "- $\\mathrm{M}$-step maximises the likelihood of the data by adjusting cluster parameters ($\\mu_k$), where we can assume that the cluster is well described by a Gaussian with a fixed variance (as shown [above](#scrollTo=NoSEEAtUIk-l\u0026line=18\u0026uniqifier=1), empirical mean is the maximum likelihood value for the $\\mu$ parameter of a Gaussian).\n",
        "\n",
        "We first explain how to implement every step of this algorithm and then write a little program to visualise the learning via $\\mathrm{K\\!-\\!means}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HARIDuUviTLk"
      },
      "source": [
        "##**Initialise the parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saQl4iN8UtFW"
      },
      "outputs": [],
      "source": [
        "# Init\n",
        "n_c = 4 # n_c is the number of clusters (K in the algorithm description)\n",
        "n_x = len(x_b)\n",
        "mu_b = np.linspace(min(x_b), max(x_b), n_c)\n",
        "\n",
        "plt.hist(x_b, bins=200, density=True, histtype='stepfilled', \n",
        "         facecolor='.9', edgecolor='.3')\n",
        "mu_colors = sns.color_palette(n_colors=n_c)\n",
        "for k_c, mu_k in enumerate(mu_b):\n",
        "  plt.axvline(mu_k, c=mu_colors[k_c])\n",
        "\n",
        "plt.title('Empirical data density and cluster means')\n",
        "plt.ylabel('p(x)')\n",
        "plt.xlabel('x (cm)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyPWGAoXiil5"
      },
      "source": [
        "##**$\\mathrm{E}$-step**\n",
        "Assign each sample to one of the clusters. \n",
        "\n",
        "We use different colours for each of the clusters. The vertical lines depict the current cluster templates and the shaded area shows the part of the input domain that was assigned to a given cluster. Assigned clusters (indexed by 0, 1, 2, 3) are plotted on the y-axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odp5VJZLSVuY"
      },
      "outputs": [],
      "source": [
        "r = np.zeros(n_x, dtype='int16')\n",
        "for k, x_k in enumerate(x_b):\n",
        "  r[k] = np.argmin(np.abs(x_k - mu_b))\n",
        "\n",
        "plt.plot(x_b, r, '.', alpha=.2, color='.2')\n",
        "for k_c, mu_k in enumerate(mu_b):\n",
        "  x_b_c = x_b[r == k_c]\n",
        "  plt.axvspan(min(x_b_c), max(x_b_c), \n",
        "              facecolor=mu_colors[k_c], alpha=0.2)\n",
        "  plt.axvline(mu_k, c=mu_colors[k_c])\n",
        "  \n",
        "plt.title('E: Assigning responsibility')\n",
        "plt.ylabel('r(x)')\n",
        "plt.xlabel('x (cm)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nibDJmVui-mq"
      },
      "source": [
        "##**$\\mathrm{M}$-step**\n",
        "Given the assignement of samples to the clusters, compute the new templates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEp3iItzTZvz"
      },
      "outputs": [],
      "source": [
        "# Maximise likelihood for each of the clusters:\n",
        "for k_c in range(n_c):\n",
        "  mu_b[k_c] = np.mean(x_b[r == k_c])\n",
        "\n",
        "plt.hist(x_b, bins=200, density=True, histtype='stepfilled', \n",
        "         facecolor='.9', edgecolor='.3')\n",
        "for k_c, mu_k in enumerate(mu_b):\n",
        "  plt.axvline(mu_k, c=mu_colors[k_c])\n",
        "plt.title('M: recompute the means')\n",
        "plt.ylabel('p(x)')\n",
        "plt.xlabel('x (cm)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szRxCpyFjLPV"
      },
      "source": [
        "##**Putting it all together**\n",
        "Let's write functions for each step of the algorithm and let's call them iteratively.  \n",
        "\n",
        "We plot cluster templates for each iteration and use transparency to depict learning progress. (Transparency is commonly referred to as `alpha` parameter in `matplotlib` functions.) This gives us a quick and computationally inexpensive way to monitor the progress of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LcboIY5YGoa"
      },
      "outputs": [],
      "source": [
        "def e_step(x, mu):\n",
        "  r = [np.nanargmin(np.abs(x_k - mu))\n",
        "      for k, x_k in enumerate(x)]      \n",
        "  return np.array(r)\n",
        "\n",
        "def m_step(x, r, cluster_ids):\n",
        "  mu_c = np.ones_like(cluster_ids) * np.nan\n",
        "  for k_c in cluster_ids:\n",
        "    x_c = x[r == k_c]\n",
        "    if np.any(x_c):\n",
        "      mu_c[k_c] = np.mean(x_c) \n",
        "  return mu_c\n",
        "\n",
        "n_steps = 10\n",
        "plt.hist(x_b, bins=200, density=True, histtype='stepfilled', \n",
        "         facecolor='.9', edgecolor='.3')\n",
        "           \n",
        "for t in range(n_steps):\n",
        "  r = e_step(x_b, mu_b)\n",
        "  mu_b = m_step(x_b, r, range(n_c))\n",
        "  for k_c, mu_k in enumerate(mu_b):\n",
        "    plt.axvline(mu_k, c=mu_colors[k_c], alpha=max(.2, t/n_steps), lw=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrDrr-WIjos5"
      },
      "source": [
        "Below we provide code to save our data and display them as an animation. Please, take a look (by double-clicking on the cell)! `matplotlib` library is truly powerful!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8vR8T6zJkYyF"
      },
      "outputs": [],
      "source": [
        "#@title Visualisation code\n",
        "def k_means(x, n_clusters, n_steps=10):\n",
        "  n_steps += 1 # Need one for the initial values.\n",
        "  mu_over_time = np.zeros((n_steps, n_clusters))\n",
        "  rlim_over_time = np.zeros((n_steps, n_clusters, 2)) * np.nan\n",
        "  mu_over_time[0] = np.random.randn(n_clusters) * np.std(x) + np.mean(x)\n",
        "  for t in range(1, n_steps):\n",
        "    r = e_step(x, mu_over_time[t - 1])\n",
        "    for k_c in range(n_clusters):\n",
        "      x_b_k = x_b[r == k_c]    \n",
        "      if len(x_b_k):\n",
        "        rlim_over_time[t, k_c] = [min(x_b_k), max(x_b_k)]  \n",
        "    mu_over_time[t] = m_step(x, r, range(n_clusters))\n",
        "  return mu_over_time, rlim_over_time\n",
        "\n",
        "def animate_kmeans(x, mu_over_time, rlim_over_time):\n",
        "  n_steps, n_clusters = np.shape(mu_over_time)\n",
        "  colors = sns.color_palette(n_colors=n_clusters)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  # Initialise graphics:\n",
        "  areas = [plt.axvspan(np.nan, np.nan, facecolor=colors[k_c], alpha=0.2)\n",
        "          for k_c in range(n_clusters)]\n",
        "  plt.hist(x, bins=200, density=True, histtype='stepfilled', \n",
        "           facecolor='.9', edgecolor='.3')\n",
        "  lines = [plt.axvline(0, c=colors[k_c])\n",
        "          for k_c in range(n_clusters)]\n",
        "\n",
        "  def animate_(frame_no): \n",
        "    frame_type = frame_no % 2\n",
        "    t = frame_no // 2  \n",
        "    for k_c, mu_k in enumerate(mu_over_time[t]):\n",
        "      if frame_type or frame_no == 0:\n",
        "        lines[k_c].set_data([mu_k, [0, 1]])\n",
        "      else:\n",
        "        xy = areas[k_c].get_xy()   \n",
        "        xy[:, 0] = rlim_over_time[t][k_c][0]\n",
        "        xy[2:-1, 0] = rlim_over_time[t][k_c][1]\n",
        "        areas[k_c].set_xy(xy)\n",
        "    if t == 0:\n",
        "      s_title = 'Initialise clusters' \n",
        "    else:\n",
        "      s_title = '{}. {}-Step'.format(t, ['E', 'M'][frame_type])\n",
        "    ax.set_title(s_title)     \n",
        "\n",
        "  my_anim = animation.FuncAnimation(fig, animate_, frames=2 * n_steps, \n",
        "                                interval=600)\n",
        "  plt.close()\n",
        "  return HTML(my_anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAPLTC_KJGBX"
      },
      "outputs": [],
      "source": [
        "params_over_time = k_means(x_b, n_clusters=4)\n",
        "animate_kmeans(x_b, *params_over_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-3hTHJqYB1O"
      },
      "source": [
        "##**How do we decide the number of clusters?**\n",
        "\n",
        "What happens when we try to run the $\\mathrm{K\\!-\\!means}$ algorithm with more clusters? Here, we initialise the means randomly.\n",
        "\n",
        "Feel free to modify the number of clusters or the number of steps to run the algorithm for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1QSYgT58lsk"
      },
      "outputs": [],
      "source": [
        "params_over_time = k_means(x_b, n_clusters=10, n_steps=20)\n",
        "animate_kmeans(x_b, *params_over_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdukHq7ekmcr"
      },
      "source": [
        "We see that sometimes the clusters become irrelevant (when there is no data assigned to a cluster, it vanishes). Some seem superflouous. \n",
        "Were they anywhere close to the truth? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46fpesWIpi64"
      },
      "source": [
        "##**Spoiler alert: the ground truth about our data**\n",
        "The data actually came from 10 clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rcBqbYcTlpl"
      },
      "outputs": [],
      "source": [
        "x_ = np.linspace(np.min(x_b), np.max(x_b), 1000)\n",
        "for k, c in enumerate(sns.color_palette('Paired', 10)):\n",
        "  plt.hist(x_b[R == k], bins=50, density=True, histtype='stepfilled', \n",
        "           edgecolor='none', color=c, alpha=.5)\n",
        "  plt.plot(x_, bell_curve(x_, HIDDEN_MU[k], HIDDEN_SIGMA[k]**2), c=c,\n",
        "           label=[0, 1, 5, 10, 80][k // 2])\n",
        "\n",
        "plt.title('Distributiton of heights for different ages (in years).')\n",
        "plt.ylabel('Probability density')\n",
        "plt.xlabel('Height (cm)')\n",
        "plt.legend(ncol=5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DILTPNlxYDK"
      },
      "source": [
        "The data reflect the [height statistics](https://tall.life/height-percentile-calculator-age-country/) for female and male genders across age. As you can see, there is a large overlap between these clusters. While $\\mathrm{K\\!-\\!means}$ works well for separable patterns, it stood no chance to decipher the ground truth of our example. Are there better methods to address our problem?\n",
        "\n",
        "Before you say \"neural networks\", notice that our data has a very compact description - it is a **mixture of (Gaussian) distributions**, with age and gender deciding parameters of each cluster. Because we had access to measurements only, age and gender can be considered as **latent variables** -- had we known them, they'd allow us to characterise this distribution well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT5bu54-pXnj"
      },
      "source": [
        "#**Latent variables in probabilistic modelling. $\\mathrm{EM}$ algorithm**\n",
        "In probabilistic modelling, latent variables enable a compact description of data. In our example above, such a latent variable was the id of the cluster from which our height data was drawn, $z_n\\in\\{1,\\ldots, K\\}$. However, our assignment was deterministic - only one cluster was assigned to each data sample. \n",
        "A **mixture of distributions** is a probabilistic extension of this approach, which models the beliefs over cluster assignments. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMCCpZ0Nz0it"
      },
      "source": [
        "##**Prior and Posterior beliefs**\n",
        "Beliefs can come in different flavours.\n",
        "1. We might have some **prior** belief about the clusters - some of them might be more common, some might be rare. This we express with $K$ positive values which sum to 1: $0 \\le \\pi_{k} \\le 1$ and $\\sum_k \\pi_{k}=1$. \n",
        "2. After seeing a data sample $x$, we can update our belief about the cluster assignment, calling it a **posterior** belief $P(z=k|x)$. According to the Bayes theorem, \n",
        "$$P(z=k|x) = \\frac{\\pi_k p(x|z=k)}{\\sum_k \\pi_k p(x|z=k)}$$\n",
        "where the denominator makes sure that our posterior belief sums to 1. \n",
        "\n",
        "$p(x|z=k)$ is the probability density similar to the one we talked about above, except that now we assume it was generated by the $k$-th cluster. We refer to it as **likelihood** of the data **conditioned** on cluster id. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwZcTn2UGrED"
      },
      "source": [
        "###**Notation** \n",
        "Here's a quick recap of the notation relating to the Bayes theorem.\n",
        "- $p(x|y)$ denotes conditioning, \"probability of $x$ given $y$\". \n",
        "- $p(x,y)$ denotes a **joint** distribution, \"probability of $x$ and $y$ co-occuring\". It is simply $$p(x, y)=p(y|x)p(x).$$ \n",
        "- **Marginalising** means summing (/integrating) over all possible values of a random variable, e.g. \n",
        "$$p(x)=\\sum_{k=1}^K p(x, z_k)= \\int_{y_{\\min}}^{y_\\max} p(x, y) dy$$ \n",
        "where we considered a discrete latent $z_k$ from a set of size K, and a continuous latent $y$ bounded by $y_\\min$ and $y_\\max$.\n",
        "\n",
        "Considering the above, we can rewrite our posterior as:\n",
        "$$p(z|x) = \\frac{p(x|z)p(z)}{p(x)} = \\frac{p(x, z)}{p(x)}$$\n",
        "Bayes theorem is easiest to memorise as the following symmetrical expression:\n",
        "$$p(x, z) = p(z|x)p(z) = p(x|z)p(x)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9F9Ms8MvaGl"
      },
      "source": [
        "###**Task** \n",
        "We observe a data point $x$, which could have come from either of two data sources. Source 1 generates similar samples with probability $p(x|z=1)=0.3$, source 2 rarely ever produces $x$ with that value, $p(x|z=2)=0.01$. However, source 2 is much more active than source 1, it produces 100x more data. What should be our posterior belief about how our data point was generated? What is the likelihood of our data point?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2-Aivp8imgs8"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "# Prior beliefs: 1/11, 10/11 (need to sum to 1)\n",
        "print('Prior beliefs p(z): 1/101, 100/101')\n",
        "print('Likelihood p(x): 0.3/101 + 1/101 ~ .13')\n",
        "print('Posterior p(z|x): (3/13, 10/13)')\n",
        "print('According to Bayes, data comes from the 2nd source with ~76% probability')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtMTRrqUaRYd"
      },
      "source": [
        "###**Task**\n",
        "Here's a task to improve familiarity of conditional, joint and marginal distributions. By manipulating the slider you can modify the prior on cluster freuqency $\\pi=p(z)$ and the likelihood of data generated by the first cluster $p(y|z)$. \n",
        "- How does the marginal change when cluster centers get closer together?\n",
        "- When is the posterior on cluster assignment useful?\n",
        "- What happens when you set the 1-st cluster prior to zero?\n",
        "- Can you modify the code to control the variance of the distributions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cL4RPbnTJhHw"
      },
      "outputs": [],
      "source": [
        "#@title Conditional, joint, marginal {run: \"auto\"}\n",
        "pi_1 = 0.3695 #@param{type:'slider', min:0, max:1, step:1e-4}\n",
        "mu_1 = 5.6 #@param{type:'slider', min:-20, max:20, step:1e-1}\n",
        "mu_2 = 10\n",
        "sigma_2 = 10\n",
        "pi_2 = 1 - pi_1\n",
        "\n",
        "x_ = np.linspace(-20, 20, 100)\n",
        "p_1 = bell_curve(x_, mu_1, sigma_2)\n",
        "p_2 = bell_curve(x_, mu_2, sigma_2)\n",
        "lw = 3\n",
        "_, axes = plt.subplots(1, 3, figsize=(18, 2.7))\n",
        "plt.sca(axes[0])\n",
        "plt.plot(x_, p_1, lw=lw, label='p(x|z=1)')\n",
        "plt.plot(x_, p_2, lw=lw, label='p(x|z=2)')\n",
        "plt.title('Conditional p(x|z)')\n",
        "y_lim = plt.ylim()\n",
        "\n",
        "plt.sca(axes[1])\n",
        "p_zx_1 = p_1 * pi_1\n",
        "p_zx_2 = p_2 * pi_2\n",
        "p_x = p_zx_1 + p_zx_2\n",
        "plt.plot(x_, p_zx_1, lw=lw, label='p(x,z=1)')\n",
        "plt.plot(x_, p_zx_2, lw=lw, label='p(x,z=2)')\n",
        "plt.plot(x_, p_x, lw=lw, label='p(x)')\n",
        "plt.title('Joint p(x,z),  marginal p(x)')\n",
        "y_lim = plt.ylim(y_lim)\n",
        "\n",
        "plt.sca(axes[2])\n",
        "plt.plot(x_, p_zx_1 / p_x, lw=lw, label='p(z=1|x)')\n",
        "plt.plot(x_, p_zx_2 / p_x, lw=lw, label='p(z=2|x)')\n",
        "plt.title('Conditional p(z|x)')\n",
        "\n",
        "for ax in axes:\n",
        "  plt.sca(ax)\n",
        "  plt.legend(ncol=3, loc='upper center', bbox_to_anchor=[0.5, -.3]);\n",
        "  plt.xlabel('x (cm)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_kKzGUun0SG"
      },
      "source": [
        "##**Generative model: mixture of Gaussians**\n",
        "Formally, we assume the following probabilistic model:\n",
        "\\begin{eqnarray}\n",
        "z \u0026\\sim\u0026 \\mathrm{Categorical}(\\mathbb{\\pi})\\\\\n",
        "x \u0026\\sim\u0026 \\mathrm{Normal}(\\mu_z, \\sigma^2_z)\n",
        "\\end{eqnarray}\n",
        "Which means that we believe our dataset was generated by the following process: For each sample $n$:\n",
        "1. Draw the id of the cluster from a categorical distribution: $P(z_{n}=k)=\\pi_k$. \n",
        "2. Draw the sample from the Gaussian distribution associated with the $k$-th cluster: $p(x_n|z_n=k)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2_k}} e^{\\left . -(x_n - \\mu_k)^2\\small \\right / 2\\sigma^2_k}$\n",
        "\n",
        "Taking the two together, we can write the likelihood of $x_n$ (data density according to the mixture of Gaussians) as: \n",
        "$$p(x_n) = \\sum_k \\pi_k \\frac{e^{\\left . -(x_n - \\mu_k)^2\\small \\right / 2\\sigma^2_k}}{\\sqrt{2 \\pi \\sigma^2_k}} $$\n",
        "\n",
        "\n",
        "The likelihood of observing all our data $\\mathcal{D}$:\n",
        "$$\\mathcal{L(D}|\\Theta) = \\prod_n\\sum_k \\pi_k \\frac{e^{\\left . -(x_n - \\mu_k)^2\\small \\right / 2\\sigma^2_k}}{\\sqrt{2 \\pi \\sigma^2_k}} $$\n",
        "where $\\Theta$ now includes all $\\pi_k$, $\\mu_k$ and $\\sigma^2_k$ for all K clusters.\n",
        "\n",
        "We see that the logarithm of that likelihood does not yield a simple expression, like it did in the case of a single Gaussian: \n",
        "$$\\mathcal{C(D}|\\Theta) = - \\sum_n\\log\\left[\\sum_k \\pi_k \\frac{e^{\\left . -(x - \\mu_k)^2\\small \\right / 2\\sigma^2_k}}{\\sqrt{2 \\pi \\sigma^2_k}} \\right]$$\n",
        "\n",
        "The summation over $k$ inside the logarithm makes the optimisation challenging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R0j2jAsjbUf"
      },
      "source": [
        "##**$\\mathrm{EM}$ algorithm**\n",
        "Instead of optimising $\\mathcal{C(D)}$, $\\mathrm{EM}$ minimises $\\langle\\mathcal{C(D, Z)}\\rangle_{p(Z|\\mathcal{D})}$, i.e. expected value of the negative log-likelihood of the data and hidden variables, rather than just data, with expectation taken over the posterior on latent variables ($p(Z|\\mathcal{D})$). In case of our mixture:\n",
        "$$\\langle\\mathcal{C(D, Z)}\\rangle_{p(Z|\\mathcal{D})} = - \\sum_n\\sum_{k}P(z_n=k|x_n) \\log\\left[\\pi_k \\frac{e^{\\left . -(x - \\mu_k)^2\\small \\right / 2\\sigma^2_k}}{\\sqrt{2 \\pi \\sigma^2_k}}\\right].$$\n",
        "Note, that we got rid of the summation over $k$ inside the logarithm! \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TgOLcu762gg"
      },
      "source": [
        "After deciding on the generative model and initialising its parameters, compute recursively.\n",
        "1. $\\mathrm{E}$-step: Compute the posterior over latent variables.\n",
        "\n",
        "  For the mixture, the posterior of cluster ids, also referred to as the *responsibilities* is: $$r_{nk} \\equiv P(z_n=k|x_n) = \\frac{\\pi_k p(x_n|z_n=k)}{\\sum_k \\pi_k p(x_n|z_n=k)}$$\n",
        "\n",
        "2. $\\mathrm{M}$-step: Maximise likelihood of the data according to the current posterior. \n",
        "\n",
        "  For our mixture of Gaussians, we get the following set of equations:\n",
        "  \\begin{eqnarray}\n",
        "  \\mu_k \u0026=\u0026 \\frac {1}{N_k} \\sum_n r_{nk} x_n\\\\\n",
        "  \\sigma_k^2 \u0026=\u0026 \\frac {1}{N_k} \\sum_n r_{nk} (x_n - \\mu_k)^2\\\\\n",
        "  \\pi_k \u0026=\u0026 \\frac{N_k}{N}\n",
        "  \\end{eqnarray}\n",
        "where $N_k = \\sum_n r_{nk}$ and $N$ is the number of all samples.\n",
        "\n",
        "\n",
        "$\\mathrm{EM}$ is an iterative algorithm that is sure to converge, although it might get stuck in local minima. \n",
        "\n",
        "Despite minimising $\\mathcal{C(D, Z|}\\Theta)$, rather than the log-loss $\\mathcal{C(D|}\\Theta)$, it can be shown that it never increases the latter. At the end of this section, you can read about the variational-$\\mathrm{EM}$ to further your understanding of this powerful optimisation technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrQBd8wStcAT"
      },
      "outputs": [],
      "source": [
        "def e_step_gaussian_mixture(x, mu, sigma_2, pi):\n",
        "  # Posterior probability for each of data samples belonging to each cluster:\n",
        "  r = np.array([pi_c * bell_curve(x, mu_c, sigma_2c) \n",
        "       for pi_c, mu_c, sigma_2c in zip(pi, mu, sigma_2)]).T\n",
        "  # normalising posterior to one:\n",
        "  r /= np.sum(r, -1, keepdims=True)\n",
        "  return np.array(r)\n",
        "\n",
        "def m_step_gaussian_mixture(x, r):\n",
        "  # Maximum likelihood:\n",
        "  n_k = np.sum(r, 0)\n",
        "  mu = np.sum(r * x[:, None], 0) / n_k\n",
        "  sigma_2 = np.sum(r * (x[:, None] - mu[None])**2, 0) / n_k\n",
        "  pi = n_k / np.sum(n_k)\n",
        "  return mu, sigma_2, pi\n",
        "\n",
        "def loss_gaussian_mixture(x, mu, sigma_2, pi):\n",
        "  p = np.sum([pi_c * bell_curve(x, mu_c, sigma_2c) \n",
        "       for pi_c, mu_c, sigma_2c in zip(pi, mu, sigma_2)], 0)\n",
        "  return - np.sum(np.log(p))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck7x6bhVfs4o"
      },
      "source": [
        "Let's first initialise the algorithm deterministically. We spread our guess over the means evenly, we use a reasonable guess for $\\sigma^2$ (based on the histogram above), and we use a flat prior over the frequency of clusters: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTcIcr5VmEjP"
      },
      "outputs": [],
      "source": [
        "n_c = 10\n",
        "n_x = len(x_b)\n",
        "mu_b = np.linspace(min(x_b), max(x_b), n_c)\n",
        "sigma_2b = [30] * n_c\n",
        "pi = np.ones(n_c) / n_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYqW8AMUurHm"
      },
      "source": [
        "\n",
        "\n",
        "The responsibility variable $r_{nk}$ expresses the probability of $x_n$ being generated from the $k$-th distribution. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrINOBhSmXNh"
      },
      "outputs": [],
      "source": [
        "r_b = e_step_gaussian_mixture(x_b, mu_b, sigma_2b, pi)\n",
        "mu_colors = sns.color_palette(n_colors=n_c)\n",
        "for k_c, mu_k in enumerate(mu_b):\n",
        "  plt.plot(x_b, r_b[:, k_c], '.', alpha=.4, c=mu_colors[k_c])\n",
        "  plt.axvline(mu_k, c=mu_colors[k_c])\n",
        "plt.title('E step: Responsibilities')\n",
        "plt.xlabel('x (cm)')\n",
        "plt.ylabel('p(z|x)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s17NFgFagQwQ"
      },
      "source": [
        "We see that most of our data points fall between two clusters (for each $x$, there are at least two lines $p(z_k|x)$ significantly greater than 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB0Lp5IZ3Y_d"
      },
      "source": [
        "Now, let's update the parameters ($\\mathrm{M}$-step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbmA6afLrhtt"
      },
      "outputs": [],
      "source": [
        "mu_b, sigma_2b, pi_b = m_step_gaussian_mixture(x_b, r_b)\n",
        "x_ = np.linspace(min(x_b), max(x_b), 1000)\n",
        "plt.hist(x_b, bins=200, density=True, histtype='stepfilled',\n",
        "         facecolor='.9', edgecolor='.3')\n",
        "for k_c, mu_k in enumerate(mu_b):\n",
        "  y_ = bell_curve(x_, mu_b[k_c], sigma_2b[k_c]) * pi_b[k_c]\n",
        "  plt.plot(x_, y_, c=mu_colors[k_c])\n",
        "  plt.axvline(mu_k, c=mu_colors[k_c])\n",
        "plt.title('M: recompute the means')\n",
        "plt.ylabel('p(x)')\n",
        "plt.xlabel('x (cm)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiqnpWT1hIc0"
      },
      "source": [
        "Again, we can use transparency to create a static plot of the training progress:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPzmwzt9wb5m"
      },
      "outputs": [],
      "source": [
        "plt.hist(x_b, bins=200, density=True, histtype='stepfilled',\n",
        "           facecolor='.9', edgecolor='.3')\n",
        "for k_ in range(n_steps):\n",
        "  r_b = e_step_gaussian_mixture(x_b, mu_b, sigma_2b, pi)\n",
        "  alpha = max((k_ / n_steps, .1))\n",
        "  mu_b, sigma_2b, pi_b = m_step_gaussian_mixture(x_b, r_b)\n",
        "  for k_c, mu_k in enumerate(mu_b):\n",
        "    y_ = bell_curve(x_, mu_b[k_c], sigma_2b[k_c]) * pi_b[k_c]\n",
        "    plt.plot(x_, y_, c=mu_colors[k_c], alpha=alpha)\n",
        "    plt.axvline(mu_k, c=mu_colors[k_c], alpha=alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3ZFpOMzh9KY"
      },
      "source": [
        "Or code up a little movie clip using `matlplotlib.animation` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z3r7RHUlPzIb"
      },
      "outputs": [],
      "source": [
        "#@title Visualisation code for EM\n",
        "def em_gaussian_mixture(x, n_clusters, n_steps=10):\n",
        "  n_steps += 1 # Need one for the initial values.\n",
        "  params_over_time = np.zeros((n_steps, 3, n_clusters))\n",
        "  params_over_time[0][0] = np.random.randn(n_clusters) * np.std(x) + np.mean(x)\n",
        "  params_over_time[0][1] = 50\n",
        "  params_over_time[0][2] = 1 / n_clusters\n",
        "  for t in range(1, n_steps):\n",
        "    r = e_step_gaussian_mixture(x, *params_over_time[t - 1])\n",
        "    params_over_time[t] = m_step_gaussian_mixture(x, r)\n",
        "  return params_over_time\n",
        "\n",
        "def animate_em_gaussian_mixture(x, params_over_time):\n",
        "  n_steps, _, n_clusters = np.shape(params_over_time)\n",
        "  colors = sns.color_palette(n_colors=n_clusters)\n",
        "\n",
        "  x_ = np.linspace(min(x), max(x), 1000)\n",
        "  fig, ax = plt.subplots()\n",
        "  # Initialise graphics:\n",
        "  pdfs = [plt.plot(x_, np.nan * x_, c=colors[k_c])[0]\n",
        "          for k_c in range(n_clusters)]\n",
        "  plt.hist(x, bins=200, density=True, histtype='stepfilled', \n",
        "           facecolor='.9', edgecolor='.3')\n",
        "  lines = [plt.axvline(np.nan, c=colors[k_c])\n",
        "          for k_c in range(n_clusters)]\n",
        "\n",
        "  def animate_(t): \n",
        "    mu_t, sigma_2t, pi_t = params_over_time[t]\n",
        "    for k_c, mu_k in enumerate(mu_t):\n",
        "      lines[k_c].set_data([mu_k, [0, 1]])\n",
        "      y_ = bell_curve(x_, mu_k, sigma_2t[k_c]) * pi_t[k_c]\n",
        "      pdfs[k_c].set_data(x_, y_)\n",
        "\n",
        "    if t == 0:\n",
        "      s_title = 'Initialise clusters' \n",
        "    else:\n",
        "      s_title = '{}'.format(t)\n",
        "    ax.set_title(s_title)     \n",
        "\n",
        "  my_anim = animation.FuncAnimation(fig, animate_, frames=n_steps, \n",
        "                                interval=600)\n",
        "  plt.close()\n",
        "  return HTML(my_anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUzEMRzSRSDn"
      },
      "outputs": [],
      "source": [
        "params_over_time = em_gaussian_mixture(x_b, n_clusters=4, n_steps=20)\n",
        "animate_em_gaussian_mixture(x_b, params_over_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB1mDr6Kz-zh"
      },
      "source": [
        "**Task**: Commonly, ML researchers visualise the progress of training by plotting how the loss changes over time. Can you do it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyGdqjJ2PRbH"
      },
      "source": [
        "###**Extra reading: $\\mathrm{EM}$ as a special case of variational-$\\mathrm{EM}$** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fM_qYEW4ZD4"
      },
      "source": [
        "So far, we used the cost function terminology. In this section, it will be easier to talk about the log-likelihood instead, $\\log(P(D|\\Theta))=-\\mathcal{C(D|}\\Theta)$, also referred to as the **evidence**. \n",
        "\n",
        "Thus, we can say that $\\mathrm{EM}$ maximises a variational evidence lower-bound, \n",
        "$$\\mathrm{ELBO}=\\log(P(D|\\Theta))-\\mathrm{KL}[P(\\mathcal{Z|D})||Q(\\mathcal{Z})],$$\n",
        "where $Q(\\mathcal{Z})$ is some distribution and $\\mathrm{KL}$ stands for the **Kullback-Leibler divergence**, which measures distances between distributions. \n",
        "\n",
        "As with all distance measures, $\\mathrm{KL}$ cannot be negative. Thus, $\\mathrm{ELBO}$ is less or equal to the evidence. \n",
        "\n",
        "To summarise:\n",
        "1. $\\mathrm{E}$-step maximises $\\mathrm{ELBO}$ wrt $Q(\\mathcal{Z})$.\n",
        "\n",
        "  This means minimising $\\mathrm{KL}[P(\\mathcal{Z|D})||Q(\\mathcal{Z})]$. \n",
        "\n",
        "\n",
        "2. $\\mathrm{M}$-step maximises $\\mathrm{ELBO}$ assuming a fixed $Q(\\mathcal{Z})$ distribution. \n",
        "\n",
        "  By recalling definitions of the  $\\mathrm{KL}$-divergence:\n",
        "\\begin{align}\n",
        "\\mathrm{KL}[P(\\mathcal{Z|D})||Q(\\mathcal{Z})]\u0026=- \\langle \\log(P(\\mathcal{Z|D}))\\rangle_{Q(\\mathcal{Z})} + \\langle \\log Q(\\mathcal{Z})\\rangle_{Q(\\mathcal{Z})},\n",
        "\\end{align}\n",
        "and of the joint distribution $P(\\mathcal{Z,D})=P(\\mathcal{Z|D})P(\\mathcal{D})$, i.e.:\n",
        "\\begin{align}\n",
        "\\log(P(\\mathcal{D})) = \\log(P(\\mathcal{Z,D})) - \\log(P(\\mathcal{Z|D}))\n",
        "\\end{align} \n",
        "we get: \n",
        "$$\\mathrm{ELBO}= \\langle \\log(P(\\mathcal{D,Z})|\\Theta))\\rangle_{Q(\\mathcal{Z})} - \\langle \\log Q(\\mathcal{Z})\\rangle_{Q(\\mathcal{Z})}.$$\n",
        "  For a fixed $Q(\\mathcal{Z})$, this means maximising \n",
        "  $\\langle \\log(P(\\mathcal{D,Z})|\\Theta))\\rangle_{Q(\\mathcal{Z})}$.\n",
        "  \n",
        "Comparing to the $\\mathrm{EM}$ algorithm introduced above, we note:\n",
        "1. The minimum of $\\mathrm{KL}$ is for $Q(\\mathcal{Z}) = P(\\mathcal{Z|D})$.\n",
        "2. Maximising $\\langle \\log(P(\\mathcal{Z|D})|\\Theta))\\rangle_{Q(\\mathcal{Z})}$ is equivalent to minimising $\\langle -\\log(P(\\mathcal{Z|D})|\\Theta))\\rangle_{Q(\\mathcal{Z})}= \\langle \\mathcal{C(D, Z|}\\Theta)\\rangle_{Q(\\mathcal{Z})}$. Thus, $\\mathrm{M}$-step is identical in both algorithms, with $Q(\\mathcal{Z})=P(\\mathcal{Z|D})$.\n",
        "\n",
        "The variational approach is more flexible than $\\mathrm{EM}$ algorithm, as it doesn't require computing the true posterior $P(\\mathcal{Z|D})$. While  $P(\\mathcal{Z|D})$ is not computable for most practical application, it is always feasible to optimise parameters of its proxy, $Q(\\mathcal{Z})$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nits21YIkI_h"
      },
      "source": [
        "#**Latent code via Neural networks: Autoencoder.**\n",
        "\n",
        "So far, we assumed the meaning for our latent variables ($z_n$ standing for the id of the cluster that generated $n$-th sample). In the next section, we talk about how the latent code can be used for compression. But before we move on to this task, let's talk about discovering the latent code using neural networks.\n",
        "\n",
        "We are going to code up the classical **autoencoder**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R65W42yXRDQX"
      },
      "source": [
        "**Notation** We use bold symbols to represent vectors. For example,  $\\mathbf{x}=(x_{1}, x_{2}, \\ldots, x_i, \\ldots, x_{D})$ represents a $D$-dimensional vector $\\mathbf{x} \\in\\mathcal{X}^D$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x14qKszYu8TV"
      },
      "source": [
        "\n",
        "An autoencoder is composed of:\n",
        "-  an **encoder**, which is a feedforward network computing a latent code $\\mathbf{z(\\mathbf{x})} \\in\\mathcal{Z}^K$ for input $\\mathbf{x} \\in\\mathcal{X}^D$, and \n",
        "- a **decoder**, which attempts to reconstruct the input $\\mathbf{x}$ from the latent code, $\\mathbf{\\hat x}(\\mathbf{z})\\in\\mathcal{X}^D$. \n",
        "\n",
        "\n",
        "It is common to use the real space $\\mathbb R$ for both $\\mathcal{X}$ and $\\mathcal{Z}$, but in the example below, we bound the activities between 0 and 1.\n",
        "\n",
        "\n",
        "An autoencoder uses a square loss function as a measure of distance between the original $\\mathbf{x}$ and its reconstruction $\\mathbf{\\hat x}$:\n",
        "$$C(\\mathbf{x}, \\mathbf{\\hat x}) = \\frac 1 2 \\sum_{i=1}^D (\\hat x_{i} - x_i)^2$$\n",
        "\n",
        "Indexing samples by $n$ and using $\\mathbf{y}$, rather than $\\mathbf{\\hat x}$ as the network output, we get the full expression of the cost the neural network has to optimise:\n",
        "$$C(\\mathcal{D}) = \\frac 1 2 \\sum_{n=1}^N \\sum_{i=1}^D (y_{i}(\\mathbf{x}_{n}) - x_{ni})^2$$\n",
        "\n",
        "**Task**: Do you recognise this loss? How can we interpret the autoencoder from the probabilistic modelling perspective?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP-S1KPEWnRG"
      },
      "source": [
        "Let us get our data. How many dimensions does each sample $\\mathbf x_n$ have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw5q0XjIxPBn"
      },
      "outputs": [],
      "source": [
        "x_c = get_data_c()\n",
        "x_c = np.reshape(x_c, (x_c.shape[0], -1, 3))\n",
        "plt.imshow(x_c); \n",
        "plt.grid(False);\n",
        "plt.ylabel('Sample id');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp83Epf0XJhX"
      },
      "source": [
        "If you answered $D=6$, then you got tricked! Colour is composed of RGB values, so the network needs to work with $D=6\\times 3$ dimensions in this case. It is best to reshape our data to directly reflect that. This is how the network \"sees\" the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EaWY4A0YAe0"
      },
      "outputs": [],
      "source": [
        "n_x = np.prod(np.shape(x_c)[1:])\n",
        "x_c = np.reshape(x_c, (-1, n_x))\n",
        "plt.imshow(x_c); \n",
        "plt.grid(False)\n",
        "plt.ylabel('Sample id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zG4PaFwTpjl"
      },
      "source": [
        "##**Multi-layer perceptron (MLP)**\n",
        "\n",
        "For the purpose of this exercise, we will construct one of the simplest autoencoders - a multi-layer perceptron (MLP) with a single hidden layer.\n",
        "\n",
        "1. The input to our network is $D$-dimensional, $\\mathbf{x}\\in \\mathbb{R}^D$.\n",
        "2. Our first layer computes the latent code $\\mathbf{z}\\in \\mathbb{R}^K_+$. In Machine Learning language, our hidden layer consists of K neurons, their activity is non-negative. \n",
        "\n",
        "  A perceptron layer consist of: \n",
        "  - a linear operation: the input is multiplied via a **weight matrix** $w^e\\in \\mathbb{R}^{K \\times D}$, and a **bias vector**  $b^e\\in\\mathbb{R}^K$ is added, \n",
        "$$z_k^{lin} = \\sum_{j=1}^D w^e_{kj} x_j + b^e_k $$\n",
        "  - followed by a nonlinear **activation function** $\\sigma$ applied to every element of the resulting vector. \n",
        "$$z_k = \\sigma(z_k^{lin}) = \\sigma(\\sum_j w^e_{kj} x_j + b^e_k) $$\n",
        "3. Our second layer is the output layer, which follows the same set of operations. \n",
        "$$y_l = \\sigma(\\sum_k w^d_{lk} z_j + b^d_l) \\equiv \\sigma(y_l^{lin})$$\n",
        " \n",
        "We choose the standard sigmoid function for our nonlinearity:\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}},$$ other popular choices are rectification (setting negative values to zero) or $\\tanh$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqqx0UYHZqf2"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def perceptron(x, w, b):\n",
        "  return sigmoid(np.dot(w, x) + b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38fZMeaBZFmC"
      },
      "outputs": [],
      "source": [
        "n_hidden = 5\n",
        "n_all_epochs = 0\n",
        "w_e = np.random.randn(n_hidden, n_x) / np.sqrt(n_x)\n",
        "w_d = np.random.randn(n_x, n_hidden) / np.sqrt(n_hidden)\n",
        "b_e = np.zeros(n_hidden)\n",
        "b_d = np.zeros(n_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5MqecJgbNeD"
      },
      "outputs": [],
      "source": [
        "def visualise_predictions(data):\n",
        "  data = np.reshape(data, (-1, n_x))\n",
        "  xhat = np.zeros_like(data)\n",
        "  for n, x_ in enumerate(data):\n",
        "    xhat[n] = perceptron(perceptron(x_, w_e, b_e), w_d, b_d)\n",
        "    \n",
        "  _, axes = plt.subplots(1, 2, figsize=(3, 4))\n",
        "  for ax, x_, a_label in zip(axes, [data, xhat], ['Data', 'AE Output']):\n",
        "    ax.imshow(x_.reshape(-1, int(n_x / 3), 3))\n",
        "    ax.grid(False)\n",
        "    ax.set_title(a_label)\n",
        "  axes[0].set_ylabel('Sample id')\n",
        "  axes[1].set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxq6Ba1v-ASF"
      },
      "source": [
        "Let's visualise predictions in a randomly initialised network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzihfAeUbp3l"
      },
      "outputs": [],
      "source": [
        "visualise_predictions(x_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kx9C4y__zBW"
      },
      "source": [
        "##**Stochastic gradient descent ($\\mathrm{SGD}$)**\n",
        "We will now derive equations for learning. \n",
        "\n",
        "According to the **gradient descent** approach, parameters of our network are modified in the direction opposite to the gradient of the cost with respect to these parameters. That is, for any parameter $\\theta$, the update $\\Delta \\theta$ is computed as \n",
        "$$\\Delta \\theta = - \\lambda \\frac{\\partial \\mathcal C(\\mathcal D)}{\\partial \\theta}$$\n",
        "where $\\lambda$, the learning rate, is some small positive number and $\\frac{\\partial \\mathcal C(\\mathcal D)}{\\partial \\theta}$ is the gradient (the derivative) of the cost wrt $\\theta$. In a single learning step, we add such updates to the current values of the parameters. For small enough updates, this guarantees that our loss decreases with every step.\n",
        "\n",
        "In the famous **stochastic gradient descent ($\\mathrm{SGD}$)** algorithm, instead of computing the full cost function $\\mathcal C(\\mathcal D)$, updates are computed and applied after a single input sample. That makes things a bit easier! However, it comes at a cost: We are losing the guarantee that every optimisation step will decrease the overall loss. The training curve might look a bit noisier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c4mMEoLSIHd"
      },
      "source": [
        "###**Task** \n",
        "Code up the square loss and its gradient for a single sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H_-nqLiSHMK"
      },
      "outputs": [],
      "source": [
        "def square_loss(x, y):\n",
        "  return \n",
        "\n",
        "def square_loss_gradient(x, y):\n",
        "  return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j_s7aKxEhU3"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "def square_loss(x, y):\n",
        "  return (x - y)** 2 / 2\n",
        "\n",
        "def square_loss_gradient(x, y):\n",
        "  return x - y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2fK6sHd_p1E"
      },
      "source": [
        "If you coded up the square loss and its gradient correctly, you should get the following graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqMuw8qBESEg"
      },
      "outputs": [],
      "source": [
        "x_ = np.linspace(-5, 5, 100)\n",
        "plt.plot(x_, square_loss(x_, 0), label=r'$L(x, 0)$')\n",
        "plt.plot(x_, square_loss_gradient(x_, 0), label=r'$dL(x, 0)/dx$')\n",
        "plt.xlabel('x')\n",
        "plt.legend()\n",
        "plt.title('Square loss and its gradient');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuxAQcU9HcSO"
      },
      "source": [
        "##**Chain rule for differentiation**\n",
        "\n",
        "In order to compute the derivative $\\frac{\\partial \\mathcal C(\\mathbf x)}{\\partial \\theta}$, we need to apply the chain rule for differentation. \n",
        "\n",
        "For a chain of functions in which $y()$ is applied to $z()$, which in turn is applied to $x$, $y(z(x))$, the gradient of $y$ with respect to $x$ can be expressed as:  \n",
        "$$\\frac{d y(z(x))}{dx} = \\frac{dy}{dz} \\frac{dz}{dx}.$$\n",
        "\n",
        "If $y$ is a function of more variables, we need to take all of them into account:\n",
        "$$\\frac{d y(z_1(x), z_2(x), \\ldots, z_k(x)))}{dx} = \\sum_k \\frac{\\partial y}{\\partial z_k} \\frac{dz_k}{dx}.$$\n",
        "(The symbol $\\partial$ is used whenever we compute a gradient of a multi-variable function.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKKPn3ebc0O1"
      },
      "source": [
        "#####**Task** \n",
        "Using the chain rule and the following basic derivatives: \n",
        "$$\\frac{d x^k}{dx} = kx^{k-1}\\qquad \\frac{d e^x}{dx}=e^x\\qquad \\frac{d (a+bx)}{dx} = b,$$ derive the gradient of $\\sigma(x) = \\frac{1}{1 + e^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_yWKqXAca2j"
      },
      "outputs": [],
      "source": [
        "def sigmoid_gradient(x):\n",
        "  return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "htYON_5PxhX1"
      },
      "outputs": [],
      "source": [
        "#@title **Solution**: `sigmoid_gradient` \n",
        "def sigmoid_gradient(x):\n",
        "  return sigmoid(x) * sigmoid(-x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52Mj_QzWUGfI"
      },
      "source": [
        "If you coded up the sigmoid correctly, our AE should be using the following activation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VxxoCAVzNIX"
      },
      "outputs": [],
      "source": [
        "x_ = np.linspace(-10, 10, 100)\n",
        "plt.plot(x_, sigmoid(x_), label=r'$\\sigma(x)$')\n",
        "plt.plot(x_, sigmoid_gradient(x_), label=r'$d\\sigma(x)/dx$')\n",
        "plt.xlabel('x')\n",
        "plt.legend()\n",
        "plt.title('Sigmoid activation function and its gradient');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMP6_XKASZ86"
      },
      "source": [
        "##**$\\mathrm{Backprop}$**\n",
        "$\\mathrm{Backprop}$ is nothing else but application of chain rule for differentation. \n",
        "\n",
        "Let us start with the decoder:\n",
        "\\begin{align}\n",
        "\\frac{\\partial C}{\\partial b_j^d} \u0026= \\sum_k \\frac{\\partial C}{\\partial y_k} \\frac{\\partial y_k}{b_j^d} = (y_j - x_j) \\sigma'(y_j^{lin})\\\\\n",
        "\\frac{\\partial C}{\\partial w^d_{jl}} \u0026=  \\sum_k \\frac{\\partial C}{\\partial y_k} \\frac{\\partial y_k}{\\partial w^d_{jl}} = (y_j - x_j) \\sigma'(y_j^{lin}) z_l\n",
        "\\end{align}\n",
        "Notice, how the summation over $k$ yields only one non-zero component, because only $y_j$ depends on $b^d_j$ (or $w^d_{jl}$).\n",
        "\n",
        "Let us now compute the gradients for the layer below, the encoding layer:\n",
        "\\begin{align}\n",
        "\\frac{\\partial C}{\\partial b^e_{j}} \u0026=  \\sum_k\\frac{\\partial C}{\\partial y_k} \\frac{\\partial y_k}{\\partial z_j} \\frac{\\partial z_j}{\\partial b^e_{j}}\n",
        "= \\frac{\\partial C}{\\partial z_j^{lin}}\\\\\n",
        "\\frac{\\partial C}{\\partial w^e_{jl}} \u0026=  \\sum_k\\frac{\\partial C}{\\partial y_k} \\frac{\\partial y_k}{\\partial z_j} \\frac{\\partial z_j}{\\partial w^e_{jl}}\n",
        "= \\frac{\\partial C}{\\partial z_j^{lin}}x_l\n",
        "\\end{align}\n",
        "Notice how for neuron $j$, gradients over its parameters are expressed via gradients over its linear activity, $z_j^{lin}$. These can be easily computed by re-using computations from the layer above:\n",
        "$$\\frac{\\partial C}{\\partial z_j^{lin}}=\\sum_k\\frac{\\partial C}{\\partial y_k}\\frac{\\partial y_k}{\\partial z_j}\\sigma'(z^{lin})=\\sum_k\\frac{\\partial C}{\\partial y_k}w^d_{kj}\\sigma'(z^{lin}_j).$$\n",
        "(In our case, the layer above was the output layer, so we re-used $\\frac{\\partial C}{\\partial y_k}$.)\n",
        "\n",
        "That's all you need to know to understand backprop (and automated differentiation in JAX or TensorFlow). Oh, and also solution of the task above:\n",
        "\\begin{align}\n",
        "\\frac{d \\sigma(z)}{dz} = \\sigma(z)\\sigma(-z)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L01yXLYsdgeY"
      },
      "source": [
        "##**Training**\n",
        "Now that we have all the ingredients in place, we can train our network.\n",
        "\n",
        "For the purpose of interepretability, we will not train the biases in this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtF4WFpD9nIZ"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-2\n",
        "n_epochs = 5_000\n",
        "loss_c = np.zeros(n_epochs)\n",
        "for epoch in range(n_epochs):\n",
        "  loss_ = []\n",
        "  for x_ in x_c:\n",
        "    z = sigmoid(np.dot(w_e, x_) + b_e)\n",
        "    y = sigmoid(np.dot(w_d, z) + b_d)\n",
        "    dL_dylin =  (y - x_) * sigmoid_gradient(y)\n",
        "    # Backprop uses gradient from the layer above:\n",
        "    dL_dzlin = np.matmul(dL_dylin, w_d) * sigmoid_gradient(z)\n",
        "    # Gradients for the weights are outer products\n",
        "    dL_dw_d = dL_dylin[:, None] * z[None]\n",
        "    dL_dw_e = dL_dzlin[:, None] * x_[None]\n",
        "    # Learning:\n",
        "    w_d -= learning_rate * dL_dw_d\n",
        "    w_e -= learning_rate * dL_dw_e\n",
        "    # For the purpose of analysis, we will not train the biases:\n",
        "    # b_d -= learning_rate * dL_dylin\n",
        "    # b_e -= learning_rate * dL_dzlin\n",
        "    loss_.append(np.sum((x_ - y)**2 / 2))\n",
        "  n_all_epochs += 1\n",
        "  loss_c[epoch] = np.sum(loss_)\n",
        "  \n",
        "plt.plot(loss_c)\n",
        "s_title = f'Training curve, last {n_epochs} epochs out of {n_all_epochs};'\n",
        "plt.title(s_title + r' $\\lambda$={}'.format(learning_rate))\n",
        "plt.xlabel('Training steps')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "visualise_predictions(x_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km1165WVj8xJ"
      },
      "source": [
        "####**Task** \n",
        "Feel free to re-run the training cell, maybe change a learning rate? The loss is likely to keep decreasing forever (or at least until numerical instabilities kick in), as we are dealing with sigmoids $\\sigma(x)$, which can obtain binary values only in the limit ($\\lim_{x\\rightarrow-\\infty}\\sigma(x)=0$, $\\lim_{x\\rightarrow\\infty}\\sigma(x)=1$). That would require infinite values for the weights in our network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyMBo9v8r6MD"
      },
      "source": [
        "####**Extra Task: Linear encoder**\n",
        "\n",
        "Using the code above, create a linear autoencoder. (You need to remove all non-linearities from the network and update the training accordingly.) The linear autoencoder is shown to be equivalent to the PCA (Principal Components Analysis). How well does it do encoding our 18 orthogonal samples?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwHmoiypTqJW"
      },
      "source": [
        "##**Analysis**\n",
        "We were able to train the network to encode our 18 orthogonal samples with 5-dimensional code. How did the network do that? Did the weights grow into inifity? Let's visualise them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzmLadw-J784"
      },
      "outputs": [],
      "source": [
        "_, axes = plt.subplots(1, 2)\n",
        "for ax, w_, l_ in zip(axes, [w_e, w_d.T], ['Encoder', 'Decoder']):\n",
        "  sns.heatmap(w_, ax=ax, center=0, cmap='RdBu_r')\n",
        "  ax.set_title(l_ + ' weights')\n",
        "axes[0].set_ylabel('Latent code id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLtpKx_llH6-"
      },
      "source": [
        "Note, that for convenience, we plot the transpose of the decoder weights. The encoder appears to have more extreme values of the weights. Why?\n",
        "\n",
        "It might be informative to pass the weights through the activation function used by the network. Each entry $\\sigma(w_{jk})$ gives us insight into what the output of the neuron $j$ would be, if only its $k$-th input was active (and unitary). This is akin to how neuroscientists measure Receptive Fields (using spotlights to stimulate the retina and the down-stream neurons of the visual system). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW_-hRdEJwl5"
      },
      "outputs": [],
      "source": [
        "_, axes = plt.subplots(1, 2)\n",
        "for ax, w_, l_ in zip(axes, [w_e, w_d.T], ['Encoder', 'Decoder']):\n",
        "  sns.heatmap(sigmoid(w_), vmin=0, vmax=1, ax=ax, center=.5, cmap='RdBu_r')\n",
        "  ax.set_title(l_ + ' weights')\n",
        "axes[0].set_ylabel('Latent code id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKap8Sq5mmi1"
      },
      "source": [
        "This was informative! The encoder and decoder matrices start to look alike. \n",
        "\n",
        "Now, if we were to go back to the RGB coding that is actually represented in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yVyOmdjK6D0"
      },
      "outputs": [],
      "source": [
        "w_e_rgb = sigmoid(w_e.reshape((-1, 6, 3), order='f' ))\n",
        "w_d_rgb = sigmoid(w_d.T.reshape((-1, 6, 3), order='f'))\n",
        "_, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "for ax, w_, l_ in zip(axes, [w_e_rgb, w_d_rgb], ['Encoder', 'Decoder']):\n",
        "  ax.imshow(w_)\n",
        "  ax.set_title(l_ + ' weights in RGB')\n",
        "  ax.grid(False)\n",
        "  ax.set_xticks([])\n",
        "axes[0].set_ylabel('Latent code id');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4sacXbEm4Pj"
      },
      "source": [
        "Again, we can visually attest to the similarities between the weights encoding and decoding the latent code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reh1WEfDeWEb"
      },
      "source": [
        "But hey, we can visualise the compressed code directly! It shows the feat we achieved with this encoder - representing 18 orthogonal samples with just 5 latent variables. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SHHKdWyW8jk"
      },
      "outputs": [],
      "source": [
        "# Note: If you changed the code to the linear autoencoder, \n",
        "# you need to update this line:\n",
        "z = np.squeeze([perceptron(x_, w_e, b_e) for x_ in x_c])\n",
        "\n",
        "_, axes = plt.subplots(1, 2, figsize=(5, 6))\n",
        "ax = axes[0]\n",
        "ax.imshow(x_c.reshape((-1, 6, 3)))\n",
        "ax.grid(False)\n",
        "ax.set_title('Input')\n",
        "ax.set_ylabel('Sample id')\n",
        "ax = axes[1]\n",
        "sns.heatmap(z, ax=ax)\n",
        "ax.set_title('Latent code');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7LBpsTSIhk_"
      },
      "source": [
        "#**Compression**\n",
        "We talked a lot about using the latent code for compression. What do we mean by it? \n",
        "\n",
        "Let's explain it on the simplest example considered [above](#scrollTo=JuzhThGFPJ_M\u0026line=4\u0026uniqifier=1), where the latent code (cluster ids) was deterministic, i.e. only one $z_n$ was assigned to each sample $x_n$. \n",
        "- Now, we simply replace $x_n$ with its cluster id $z_n$. \n",
        "- We also need to encode the clusters, but there is much fewer of them than samples $K\\ll N$, which requires less memory. \n",
        "\n",
        "How much memory do we save?\n",
        "\n",
        "By default, `numpy` uses `float64` to encode numbers, i.e. your computer uses 64 bits per number. But to encode 4 clusters, we only need 2! The four possible clusters can be encoded by $00, 01, 10, 11$.  It's saving memory by a factor of $\\mathbf{32}$!  (To be exact, it is $64N / (2N + 64K)$, which is almost $32$.)\n",
        "And you get even more when your samples are high-dimenional. For $D$-dimenions, you can compress dataset taking $64ND$ bits of memory into $2N + 64KD$ bits. Saving such compressed data takes nearly $\\mathbf{32D}$ less space!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecx6PRDW4_1g"
      },
      "source": [
        "We will now apply the compression to an image. \n",
        "\n",
        "Our data is simply pixels. Thus, our samples are now vectors, rather than scalars ($\\mathbf{x}\\in\\mathbb{R}^3$). \n",
        "\n",
        "We use $\\mathrm{K\\!-\\! means}$ to obtain $z_n$ of each of the $N$ pixels and cluster parameters $\\mathbf{\\mu}_k$ for each of the $K$ clusters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TK1YDTKQPcG"
      },
      "outputs": [],
      "source": [
        "from skimage import data\n",
        "x = data.rocket()\n",
        "# We normalise the data between (0, 1) for easier plotting\n",
        "x = x / np.max(x)\n",
        "plt.imshow(x); plt.axis('off')\n",
        "x_shape = x.shape\n",
        "# And we reshape it so that N = number of pixels.\n",
        "N = np.prod(x_shape[:2])\n",
        "x = np.reshape(x, (N, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sSrTFdbhbiL"
      },
      "source": [
        "We need to adjust the `e_step` and `m_step` to be able to apply them to vectors. We use Euclidean distance to measure the similarity between samples and clusters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MTf3a6GVEmU"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import distance_matrix\n",
        "def e_step(x, mu):\n",
        "  d = distance_matrix(x, mu)\n",
        "  return np.nanargmin(d, 1) \n",
        "\n",
        "def m_step(x, r, cluster_ids):\n",
        "  # We change the way we treat clusters that have no data assinged to them.\n",
        "  # Rather than becoming irrelevant, they now represent a black pixel (0, 0, 0): \n",
        "  mu_c = np.zeros((len(cluster_ids), x.shape[-1]))\n",
        "  for k_c in cluster_ids:\n",
        "    x_c = x[r == k_c]\n",
        "    if np.any(x_c):\n",
        "      mu_c[k_c] = np.mean(x_c, 0) \n",
        "  return mu_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J46fdEJF8hDf"
      },
      "source": [
        "Let's apply the $\\mathrm{K\\!-\\!means}$ algorithm to our image, compute the responsibilities ($\\mathrm{E}$-step) and cluster parameters ($\\mathrm{M}$-step) and use both to compress our image! \n",
        "\n",
        "Note: What we do to the image, replacing the original pixel values by a values from a discrete set of colours is also called **image segmentation**, or **vector quantisation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWyH5qcQYAW1"
      },
      "outputs": [],
      "source": [
        "def show_mu(x, ax):\n",
        "  # cluster parameters have shape (K, D), so if the last dim codes for RGB, \n",
        "  # we need to reshape it into (1, K, 3) \n",
        "  x = x[None] if x.shape[-1] == 3 else x.T\n",
        "  ax.imshow(x)\n",
        "\n",
        "n_c = 7\n",
        "n_steps = 5\n",
        "n_rgb = x.shape[-1]\n",
        "cluster_ids = range(n_c)\n",
        "\n",
        "mu_t = np.random.uniform(size=(n_c, n_rgb))\n",
        "_, axes = plt.subplots(1, 2, figsize=(15, 2.7))\n",
        "show_mu(mu_t, axes[0])\n",
        "axes[0].set_title(r'Initial draw of $\\mu$')\n",
        "axes[1].imshow(x.reshape(x_shape))\n",
        "axes[1].set_title(r'Original image')\n",
        "[ax.axis(False) for ax in axes]\n",
        "\n",
        "r_t = e_step(x, mu_t)\n",
        "for t in range(n_steps):\n",
        "  fig, axes = plt.subplots(1, 3, figsize=(15, 2.7), gridspec_kw={'top':.9})\n",
        "  fig.suptitle(f'Step {t + 1}')\n",
        "  axes[0].imshow(r_t.reshape(x_shape[:2]), cmap='Set1')\n",
        "  axes[0].set_title(f'E step: $z_n$')\n",
        "  mu_t = m_step(x, r_t, cluster_ids)\n",
        "  show_mu(mu_t, axes[1])\n",
        "  axes[1].set_title(f'M step: $\\mu_k$')\n",
        "  axes[2].imshow(mu_t[r_t].reshape(x_shape))\n",
        "  axes[2].set_title(f'Compressed image')\n",
        "  [ax.axis(False) for ax in axes]\n",
        "  plt.show()\n",
        "  r_t = e_step(x, mu_t) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BE8aXEe-I-Y"
      },
      "source": [
        "####**Task**\n",
        "\n",
        "Try out a different number of clusters, you can also let the algorithm run for longer. You could also try a different image from the `skimage.data` library, or maybe even upload your own photo?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhDF8ZvXero3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "aMCCpZ0Nz0it",
        "4R0j2jAsjbUf"
      ],
      "name": "Introduction to unsupervised learning",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
