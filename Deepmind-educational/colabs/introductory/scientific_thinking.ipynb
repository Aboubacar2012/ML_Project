
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VKgQQ2G2yGx"
      },
      "source": [
        "# Scientific Thinking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Xi9ideY0sL"
      },
      "source": [
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003eCopyright 2021 DeepMind Technologies Limited.\u003c/p\u003e\n",
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003e Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at \u003c/p\u003e\n",
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003e \u003ca href=\"https://www.apache.org/licenses/LICENSE-2.0\"\u003ehttps://www.apache.org/licenses/LICENSE-2.0\u003c/a\u003e \u003c/p\u003e\n",
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003e Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. \u003c/p\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Z6XyFh26fy"
      },
      "source": [
        "**Aim**\n",
        "This colab intends to teach you some basic ideas about scientific thinking! The goal is to develop an intuition for some of the common pitfalls that we, as scientists and humans, often encounter when trying to understand our world.\n",
        "\n",
        "**Disclaimer**\n",
        "\n",
        "This code is intended for educational purposes and, in the name of usability for a non-technical audience, it does not always follow best practices for software engineering.\n",
        "\n",
        "\n",
        "**Links to resources**\n",
        "- [What is Colab?](https://colab.sandbox.google.com/notebooks/intro.ipynb) If you have never used Colab before, get started here!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaq9CBiRuit2"
      },
      "source": [
        "## Setting up this Colab notebook.\n",
        "\n",
        "To install all the dependencies that you need to run this colab run the cell `Setting up the notebook` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNvvC_4luit4"
      },
      "outputs": [],
      "source": [
        "#@title Setting up the notebook\n",
        "#@markdown \u003e Installing and importing dependencies, as well as defining the code we'll use throughout the colab\n",
        "\n",
        "\n",
        "# Installing dependencies\n",
        "\n",
        "print(\"Installing dependencies...\", end='')\n",
        "\n",
        "from IPython.utils import io\n",
        "\n",
        "with io.capture_output() as captured:\n",
        "  # Add all the pip installs of modules necessary to run the colab\n",
        "  %reset -f\n",
        "\n",
        "  !apt-get update\n",
        "  !apt-get install pip\n",
        "  !pip install pyvirtualdisplay\n",
        "\n",
        "  # You can directly pip install packages\n",
        "  # e.g. !pip install dm-acme\n",
        "  # or you can clone the repo from GithHub\n",
        "  # e.g. !git clone https://github.com/deepmind/acme\n",
        "  !pip install pycolab\n",
        "\n",
        "print(\"DONE!\")\n",
        "\n",
        "\n",
        "print(\"Importing dependencies...\", end='')\n",
        "\n",
        "# Importing dependencies\n",
        "\n",
        "import base64\n",
        "import io\n",
        "import math\n",
        "import uuid\n",
        "import warnings\n",
        "\n",
        "from google.colab import html\n",
        "from google.colab import output\n",
        "import IPython\n",
        "import numpy as np\n",
        "from pycolab import ascii_art\n",
        "from pycolab import things as plab_things\n",
        "from pycolab.prefab_parts import sprites as prefab_sprites\n",
        "\n",
        "from PIL import Image, ImageEnhance, ImageDraw\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"DONE!\")\n",
        "\n",
        "print(\"Defining helper functions...\", end='')\n",
        "\n",
        "\n",
        "# Helper code\n",
        "\n",
        "# @title Helper code.\n",
        "# Brown: [0.219, 0.129, 0.098]\n",
        "# Purple (gem): [0.376, 0.101, 0.29]\n",
        "# Red (gem): [0.93, 0.267, 0.184]\n",
        "# Blue (path): [0.388, 0.674, 0.745]\n",
        "\n",
        "original_rgb = {\n",
        "    ' ': [0.8, 0.8, 0.8],    # Undefined / empty / off\n",
        "    '-': [0.3, 1.0, 1.0],    # Cyan empty - indicates path.\n",
        "\n",
        "    '#': [0.219, 0.129, 0.098],  # Wall\n",
        "    '@': [0.219, 0.129, 0.098],  # Fake Wall\n",
        "\n",
        "    'B': [0.376, 0.101, 0.29],   # Blue gem (purple-ish for colorblind)\n",
        "    'L': [0.376, 0.101, 0.29],   # Blue gem\n",
        "    'U': [0.376, 0.101, 0.29],   # Blue gem\n",
        "\n",
        "    'R': [0.93, 0.267, 0.184],      # Red gem\n",
        "    'E': [0.93, 0.267, 0.184],      # Red gem\n",
        "    'D': [0.93, 0.267, 0.184],      # Red gem\n",
        "\n",
        "    'F': [0.376, 0.101, 0.29],  # Fake gem\n",
        "\n",
        "    'P': [0.9, 0.9, 0.2],    # Player\n",
        "\n",
        "    1: [1.0, 0.8, 1.0],    # \"true\" / \"on\" in 0D observation\n",
        "\n",
        "    33: [0.3, 0.3, 1.0],    # Blue key\n",
        "    34: [0.3, 1.0, 0.3],    # Green key\n",
        "    35: [1.0, 0.3, 0.3],    # Red key\n",
        "    36: [1.0, 1.0, 0.3],    # Yellow key\n",
        "    37: [1.0, 0.3, 1.0],    # Magenta key\n",
        "    38: [0.3, 1.0, 1.0],    # Cyan key\n",
        "    42: [0.7, 0.7, 0.7],    # Skeleton key\n",
        "\n",
        "    49: [0.0, 0.0, 0.6],    # Blue door\n",
        "    50: [0.0, 0.6, 0.0],    # Green door\n",
        "    51: [0.6, 0.0, 0.0],    # Red door\n",
        "    52: [0.6, 0.6, 0.0],    # Yellow door\n",
        "    53: [0.6, 0.0, 0.6],    # Magenta door\n",
        "    54: [0.0, 0.6, 0.6],    # Cyan door\n",
        "\n",
        "    64: [0.9, 0.9, 0.2],    # Player\n",
        "    43: [0.9, 0.9, 0.2],    # Player\n",
        "    46: [0.0, 0.0, 0.0],    # Background\n",
        "    88: [1.0, 1.0, 1.0],    # Wall\n",
        "    62: [0.9, 0.9, 0.9],    # Goal\n",
        "\n",
        "    67: [1.0, 0.0, 0.0],    # Interrupter\n",
        "    66: [1.0, 0.5, 0.5],    # Interruption preventer\n",
        "}\n",
        "to_rgb = original_rgb.copy()\n",
        "\n",
        "KEY_MAP = {'w': 0,\n",
        "           's': 1,\n",
        "           'a': 2,\n",
        "           'd': 3,\n",
        "           'Enter': 4}\n",
        "\n",
        "def get_character_positions(level_map, character):\n",
        "  \"\"\"Returns the position of the character on the map.\"\"\"\n",
        "  indices = []\n",
        "  for row_idx, line in enumerate(level_map):\n",
        "    if character in line:\n",
        "      col_idx = line.index(character)\n",
        "      indices.append((row_idx, col_idx))\n",
        "  return indices\n",
        "\n",
        "\n",
        "def make_game(game_type='experiment', level='L1', per_timestep_reward=0.0):\n",
        "  \"\"\"Builds and returns a four-rooms game.\"\"\"\n",
        "  to_rgb = original_rgb.copy()\n",
        "  if game_type == 'experiment':\n",
        "    game_dict = EXPERIMENT_GAMES[level]\n",
        "  elif game_type == 'eval':\n",
        "    game_dict = EVAL_GAMES[level]\n",
        "\n",
        "  update_schedule = UPDATE_SCHEDULE[level]\n",
        "  drapes = {}\n",
        "  for gem_char in GEM_CHARS:\n",
        "    position = get_character_positions(game_dict, gem_char)\n",
        "    if position:\n",
        "      position = position[0]\n",
        "      gem_class = GemDrape\n",
        "      if gem_char == 'F':\n",
        "        gem_class = FakeGemDrape\n",
        "      drapes[gem_char] = ascii_art.Partial(\n",
        "          gem_class, x=position[1], y=position[0],\n",
        "          reward_val=REWARD_DICT[level][gem_char])\n",
        "  sprites = {'P': ascii_art.Partial(PlayerSprite,\n",
        "                                    per_timestep_reward=per_timestep_reward)}\n",
        "  return ascii_art.ascii_art_to_game(\n",
        "      game_dict, what_lies_beneath=' ',\n",
        "      sprites=sprites,\n",
        "      drapes=drapes,\n",
        "      update_schedule=update_schedule,\n",
        "      occlusion_in_layers=False\n",
        "      )\n",
        "\n",
        "\n",
        "def to_img(board, img_width=300, grid_color=(50, 50, 50)):\n",
        "  \"\"\"Return the map, given some obs.\"\"\"\n",
        "  img = np.asarray([[to_rgb[chr(el)] for el in row] for row in board])\n",
        "  img = (img * 255).astype('uint8')\n",
        "  h, w = img.shape[:2]\n",
        "  new_w = img_width\n",
        "  new_h = int(h*300/w)\n",
        "  img = Image.fromarray(img)\n",
        "  img = img.resize((new_w, new_h), Image.NEAREST)\n",
        "\n",
        "  d = ImageDraw.Draw(img)\n",
        "  height_offset = int(new_h/h)\n",
        "  width_offset = int(new_w/w)\n",
        "  for i in range(1, h-1):\n",
        "    line_index = i * height_offset\n",
        "    d.line([width_offset, line_index, new_w-width_offset, line_index],\n",
        "           fill=grid_color)\n",
        "  for j in range(1, w-1):\n",
        "    line_index = j * width_offset\n",
        "    d.line([line_index, height_offset, line_index, new_h-height_offset],\n",
        "           fill=grid_color)\n",
        "  return img\n",
        "\n",
        "def show_map(obs):\n",
        "  \"\"\"Display the map, given some obs.\"\"\"\n",
        "  IPython.display.display(to_img(obs.board))\n",
        "\n",
        "\n",
        "class PlayerSprite(prefab_sprites.MazeWalker):\n",
        "  \"\"\"A `Sprite` for our player.\n",
        "\n",
        "  This `Sprite` ties actions to going in the four cardinal directions. If we\n",
        "  reach a magical location (in this example, (4, 3)), the agent receives a\n",
        "  reward of 1 and the episode terminates.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, corner, position, character, per_timestep_reward=0.0):\n",
        "    \"\"\"Inform superclass that we can't walk through walls.\"\"\"\n",
        "    self._per_timestep_reward = per_timestep_reward\n",
        "    super(PlayerSprite, self).__init__(\n",
        "        corner, position, character, impassable='#', confined_to_board='True')\n",
        "\n",
        "  def update(self, actions, board, layers, backdrop, things, the_plot):\n",
        "    \"\"\"Update the player sprite position based on the action.\"\"\"\n",
        "    del layers, backdrop   # Unused.\n",
        "    the_plot.add_reward(self._per_timestep_reward)\n",
        "\n",
        "    # Apply motion commands.\n",
        "    if actions == 0:    # walk upward?\n",
        "      self._north(board, the_plot)\n",
        "    elif actions == 1:  # walk downward?\n",
        "      self._south(board, the_plot)\n",
        "    elif actions == 2:  # walk leftward?\n",
        "      self._west(board, the_plot)\n",
        "    elif actions == 3:  # walk rightward?\n",
        "      self._east(board, the_plot)\n",
        "    elif actions == 4:\n",
        "      the_plot.terminate_episode()\n",
        "\n",
        "    target_character = chr(board[self.position[0], self.position[1]])\n",
        "    # This will return None if target_character is None\n",
        "    target_thing = things.get(target_character)\n",
        "    # Inform plot of overlap between player and thing.\n",
        "    if target_thing:\n",
        "      the_plot['over_this'] = (target_character, self.position)\n",
        "\n",
        "class BoxThing(plab_things.Drape):\n",
        "  \"\"\"Base class for locks, keys and gems.\"\"\"\n",
        "\n",
        "  def __init__(self, curtain, character, x, y, reward_val=1.0):\n",
        "    super(BoxThing, self).__init__(curtain, character)\n",
        "    self._rewarded = False\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.curtain[y][x] = True\n",
        "    self.reward_val = reward_val\n",
        "\n",
        "  def where_player_over_me(self, the_plot):\n",
        "    \"\"\"Check if player is over this thing. If so, returns the coordinates.\"\"\"\n",
        "    over_this = the_plot.get('over_this')\n",
        "    if over_this:\n",
        "      character, (y, x) = over_this\n",
        "      if character == self.character and self.curtain[y][x]:\n",
        "        return y, x\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "class GemDrape(BoxThing):\n",
        "  \"\"\"The gem.\"\"\"\n",
        "\n",
        "  def update(self, actions, board, layers, backdrop, things, the_plot):\n",
        "    \"\"\"Sets a reward when the player sprite overlaps with the gem.\"\"\"\n",
        "    if self.where_player_over_me(the_plot) and not self._rewarded:\n",
        "      the_plot.add_reward(self.reward_val)\n",
        "      self._rewarded = True\n",
        "\n",
        "class FakeGemDrape(BoxThing):\n",
        "  \"\"\"The gem but reward depends on player's position.\"\"\"\n",
        "\n",
        "  def update(self, actions, board, layers, backdrop, things, the_plot):\n",
        "    \"\"\"Sets a reward whenever the player is in the second column.\"\"\"\n",
        "    for row in board:\n",
        "      # 80 is the player.\n",
        "      if 80 in row:\n",
        "        pos = np.where(row == 80)[0][0]\n",
        "    if pos == 2 and not self._rewarded:\n",
        "      the_plot.add_reward(self.reward_val)\n",
        "      self._rewarded = True\n",
        "\n",
        "# Maps\n",
        "EXPERIMENT_GAMES = {}\n",
        "EVAL_GAMES = {}\n",
        "REWARD_DICT = {}\n",
        "UPDATE_SCHEDULE = {}\n",
        "\n",
        "level = 'L1'\n",
        "EXPERIMENT_GAMES[level] = ['#############',\n",
        "                           '#B    B     #',\n",
        "                           '#           #',\n",
        "                           '#           #',\n",
        "                           '#           #',\n",
        "                           '#           #',\n",
        "                           '#           #',\n",
        "                           '#    ###    #',\n",
        "                           '#    #B#    #',\n",
        "                           '#    # #    #',\n",
        "                           '#    # #    #',\n",
        "                           '#B   #P#    #',\n",
        "                           '#############']\n",
        "REWARD_DICT[level] = {'B': 3}\n",
        "\n",
        "EVAL_GAMES[level] = ['#############',\n",
        "                     '#B    B     #',\n",
        "                     '#           #',\n",
        "                     '#           #',\n",
        "                     '#           #',\n",
        "                     '########    #',\n",
        "                     '#B-----#    #',\n",
        "                     '######-#    #',\n",
        "                     '#    #-#    #',\n",
        "                     '#    #-#    #',\n",
        "                     '#    #-#    #',\n",
        "                     '#B   #P#    #',\n",
        "                     '#############']\n",
        "REWARD_DICT[level] = {'B': 3}\n",
        "# Make sure the player gets updated first so that the gems can detect its presence.\n",
        "UPDATE_SCHEDULE[level] = ['P', 'B']\n",
        "\n",
        "level = 'L2'\n",
        "EXPERIMENT_GAMES[level] = ['#############',\n",
        "                           '#B    B     #',\n",
        "                           '#           #',\n",
        "                           '#           #',\n",
        "                           '#           #',\n",
        "                           '#           #',\n",
        "                           '#           #',\n",
        "                           '#    ###    #',\n",
        "                           '#    #R#    #',\n",
        "                           '#    # #    #',\n",
        "                           '#    # #    #',\n",
        "                           '#B   #P#    #',\n",
        "                           '#############']\n",
        "\n",
        "EVAL_GAMES[level] = ['#############',\n",
        "                     '#B    B     #',\n",
        "                     '#           #',\n",
        "                     '#           #',\n",
        "                     '#           #',\n",
        "                     '########    #',\n",
        "                     '#R-----#    #',\n",
        "                     '######-#    #',\n",
        "                     '#    #-#    #',\n",
        "                     '#    #-#    #',\n",
        "                     '#    #-#    #',\n",
        "                     '#B   #P#    #',\n",
        "                     '#############']\n",
        "REWARD_DICT[level] = {'R': 0, 'B': 0}\n",
        "UPDATE_SCHEDULE[level] = ['P', 'R', 'B']\n",
        "\n",
        "level = 'L3'\n",
        "EXPERIMENT_GAMES[level] = ['#############',\n",
        "                           '#F@   F     #',\n",
        "                           '# @         #',\n",
        "                           '# @         #',\n",
        "                           '# @         #',\n",
        "                           '# @@@@@@@@@ #',\n",
        "                           '#         @ #',\n",
        "                           '# @@@@  @@@ #',\n",
        "                           '# @  @  @   #',\n",
        "                           '# @  @  @   #',\n",
        "                           '# @  @  @   #',\n",
        "                           '#F@  @ P@   #',\n",
        "                           '#############']\n",
        "\n",
        "# EVAL_GAMES[level]  = ['#############',\n",
        "#                       '#B@   B     #',\n",
        "#                       '#          #',\n",
        "#                       '#          #',\n",
        "#                       '#          #',\n",
        "#                       '# @@@@@@@@ #',\n",
        "#                       '#  -----  @ #',\n",
        "#                       '# @@@@ -@@@ #',\n",
        "#                       '# @  @ -@   #',\n",
        "#                       '# @  @ -@   #',\n",
        "#                       '# @  @ -@   #',\n",
        "#                       '#B@   @P@   #',\n",
        "#                       '#############']\n",
        "\n",
        "\n",
        "EVAL_GAMES[level] = ['#############',\n",
        "                     '#F@   F     #',\n",
        "                     '#     -     #',\n",
        "                     '#     -     #',\n",
        "                     '#     -     #',\n",
        "                     '#     -     #',\n",
        "                     '#     --  @ #',\n",
        "                     '# @@@@ -@@@ #',\n",
        "                     '# @  @ -@   #',\n",
        "                     '# @  @ -@   #',\n",
        "                     '# @  @ -@   #',\n",
        "                     '#F@  @ P@   #',\n",
        "                     '#############']\n",
        "REWARD_DICT[level] = {'F': 10}\n",
        "UPDATE_SCHEDULE[level] = ['P', 'F']\n",
        "\n",
        "# This should allow us to render a few things with the same color/behavior (B, L, U) are blue rewarding gems.\n",
        "GEM_CHARS = ['B', 'L', 'U', 'R', 'E', 'D', 'F']\n",
        "\n",
        "# HTML stuff.\n",
        "# The HTML class is open source and allows us to create HTML class elements.\n",
        "# Here we just create an image class that allows us to play with pycolab envs easily.\n",
        "class Img(html.Element):\n",
        "  \"\"\"Helper class to visualize and update pycolab envs.\"\"\"\n",
        "  def __init__(self, obs, img_width=None, brighten=True,\n",
        "               brighten_factor=2.0, src=None):\n",
        "    self._brighten = brighten\n",
        "    self._brighten_factor = brighten_factor\n",
        "    super(Img, self).__init__('img')\n",
        "    if img_width is None:\n",
        "      img_width = 300\n",
        "    self._img_width = img_width\n",
        "    self._mask_locations = []\n",
        "    self.update_obs(obs)\n",
        "\n",
        "    if src is not None:\n",
        "      raise ValueError()\n",
        "\n",
        "  def reset_mask(self):\n",
        "    \"\"\"Resets mask to an empty list.\"\"\"\n",
        "    self._mask_locations = []\n",
        "\n",
        "  def _add_to_mask(self, obs, player_x, player_y):\n",
        "    for layer in obs.layers:\n",
        "      if layer != 'P':\n",
        "        xs, ys = np.where(obs.layers[layer] == True)\n",
        "        for x, y in zip(xs, ys):\n",
        "          if x == player_x and y == player_y:\n",
        "            self._mask_locations.append((x, y))\n",
        "\n",
        "  def _apply_mask(self, obs):\n",
        "    board = obs.board\n",
        "    for loc in self._mask_locations:\n",
        "      x, y = loc\n",
        "      # 32 is ' '\n",
        "      board[x][y] = 32\n",
        "    return board\n",
        "\n",
        "  def _update_board_with_mask(self, obs):\n",
        "    player_x, player_y = map(lambda x: x[0],\n",
        "                             np.where(obs.layers['P'] == True))\n",
        "    self._add_to_mask(obs, player_x, player_y)\n",
        "    board = self._apply_mask(obs)\n",
        "    board[player_x, player_y] = 80\n",
        "    return board\n",
        "\n",
        "  def update_obs(self, obs):\n",
        "    \"\"\"Updates the player position based on the board state.\"\"\"\n",
        "    board = self._update_board_with_mask(obs)\n",
        "    img = to_img(board)\n",
        "    content = self._to_jpeg(img)\n",
        "    url = 'data:image/jpeg;base64,'+base64.b64encode(content).decode('utf-8')\n",
        "    self.set_property('src', url)\n",
        "\n",
        "  def _to_jpeg(self, img):\n",
        "    if self._brighten:\n",
        "      enhancer = ImageEnhance.Brightness(img)\n",
        "      img = enhancer.enhance(self._brighten_factor)\n",
        "    buf = io.BytesIO()\n",
        "    img.save(buf, format=\"JPEG\",)\n",
        "    return buf.getvalue()\n",
        "\n",
        "# Another cool thing that we can do is create *any* class and update its '_repr_html'\n",
        "# method to return html code. When you call display(my_class), it runs that\n",
        "# HTML code. Below we've set up a button class that runs a call back function when\n",
        "# pressed.\n",
        "class InvokeButton(object):\n",
        "  def __init__(self, title, callback):\n",
        "    self._title = title\n",
        "    self._callback = callback\n",
        "\n",
        "  def _repr_html_(self):\n",
        "    callback_id = 'button-' + str(uuid.uuid4())\n",
        "    output.register_callback(callback_id, self._callback)\n",
        "\n",
        "    template = \"\"\"\u003cbutton id=\"{callback_id}\"\u003e{title}\u003c/button\u003e\n",
        "        \u003cscript\u003e\n",
        "          document.querySelector(\"#{callback_id}\").onclick = (e) =\u003e {{\n",
        "            google.colab.kernel.invokeFunction('{callback_id}', [], {{}})\n",
        "            e.preventDefault();\n",
        "          }};\n",
        "        \u003c/script\u003e\"\"\"\n",
        "    html = template.format(title=self._title, callback_id=callback_id)\n",
        "    return html\n",
        "\n",
        "class Div(html.Element):\n",
        "  def __init__(self):\n",
        "    super(Div, self).__init__('input')\n",
        "\n",
        "  @property\n",
        "  def text_content(self):\n",
        "    return self.get_property('textContent')\n",
        "\n",
        "  @text_content.setter\n",
        "  def text_content(self, value):\n",
        "    return self.set_property('textContent', value)\n",
        "\n",
        "class GameKeeper():\n",
        "  \"\"\"Maintains game state and updates the board when player moves.\"\"\"\n",
        "  def __init__(self, game_type, level, per_timestep_reward=0.0):\n",
        "    self._game_type = game_type\n",
        "    self._level = level\n",
        "    self._per_timestep_reward = per_timestep_reward\n",
        "    self._start_game()\n",
        "\n",
        "  def _start_game(self):\n",
        "    self.game = make_game(self._game_type, self._level,\n",
        "                          self._per_timestep_reward)\n",
        "    self.obs, r, _ = self.game.its_showtime()\n",
        "    self._return = 0.\n",
        "\n",
        "  def move(self, key):\n",
        "    \"\"\"Moves the player based on the key pressed.\"\"\"\n",
        "    action = KEY_MAP[key]\n",
        "    if self._game_type == 'eval':\n",
        "      print(\"Heeey now :) Let us be sciencey and keep the experiments to the lab!\")\n",
        "      return\n",
        "    self.obs, reward, _ = self.game.play(action)\n",
        "    if self.game.game_over:\n",
        "      print(\"Game over! The reward was: \", self._return)\n",
        "      print(\"Restarting game...\")\n",
        "      self._start_game()\n",
        "      img.reset_mask()\n",
        "    else:\n",
        "      self._return += reward\n",
        "    img.update_obs(self.obs)\n",
        "\n",
        "print(\"DONE!\")\n",
        "\n",
        "print(\"\\nSetup DONE!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeAEjRiScOFB"
      },
      "source": [
        "# What is scientific thinking?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH48Ug41FqY_"
      },
      "source": [
        "Science is an organised way to form knowledge about... well, everything around us. It is the way for us to understand the world by observing it.\n",
        "\n",
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/scientific-thinking/what_is_thinking.png\" alt=\"drawing\" height=\"350\"/\u003e\n",
        "\u003c/center\u003e\n",
        "\n",
        "Humankind has been building on the knowledge of our ancestors for thousands of years, often not thinking too much about the origins of that knowledge: why is it that people think like that, or what might happen if we think differently?\n",
        "We use this knowledge to do all sorts of wonderful things!\n",
        "Knowledge helps us better understand the world and humanity, for example, understand how spiders weave their webs, or how groups of people behave in emergency situations.\n",
        "Knowledge also helps us to develop technology to continually improve our standard of living, for example, invent electricity, central heating, or phones!\n",
        "For the most part though, this process of building knowledge was not done in a structured and systematic way.\n",
        "Only fairly recently, relative to the timespan of humanity, have we started thinking about the core elements of how to form knowledge, how to test that knowledge, and good methods that we can use to do this.\n",
        "\n",
        "This was formalised as the scientific method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKsguvaxei6a"
      },
      "source": [
        "## The scientific method\n",
        "\n",
        "The scientific method is based on a hands-on approach to knowledge development. It consists of three steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lMqv67QF53s"
      },
      "source": [
        "**Step 1:** Make an **observation** and then come up with a **testable** and **falsifiable** hypothesis that helps explain it.\n",
        "\n",
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/scientific-thinking/method_step_1.png\" alt=\"drawing\" height=\"350\"/\u003e\n",
        "\u003c/center\u003e\n",
        "\n",
        "\n",
        "\n",
        "Thats a lot of fancy terms! Let's try to understand what they mean a little better:\n",
        "\n",
        "\n",
        "*   **Observation**: To observe is to actively inspect something, in order to collect information about it. For example *collecting information about the behaviour of falling stones* is conducting an observation, while just *looking at the stones and admiring how powerful they are when they fall* is not.\n",
        "\n",
        "*   **Testable**: A hypothesis is testable if we can, in some realistic conditions (practicality, plausibility), decide whether it is true or false. For example, *two different-sized rocks released from the same height will fall to the ground at the same time* is a testable hypothesis, whereas the same hypothesis for rocks on the sun is not testable (yet---it burns!).\n",
        "\n",
        "*   **Falsifiable**: A hypothesis is falsifiable if it *can be disproved by evidence*. For example our hypothesis about different-sized rocks falling to the ground at the same time is also *falsifiable*. We can just measure the time for them to fall to see if it is true!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oJrQlIgrYCj"
      },
      "source": [
        "Phew! OK! Now that we have our hypothesis, what do we do with it? That brings us to..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFiSvorQGpsS"
      },
      "source": [
        "**Step 2**: Design an **experiment** to test this hypothesis and draw conclusions by **analysing** the results.\n",
        "\n",
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/scientific-thinking/method_step_2.png\" alt=\"drawing\" height=\"350\"/\u003e\n",
        "\u003c/center\u003e\n",
        "\n",
        "\n",
        "Let's try to define these terms:\n",
        "\n",
        "- **Experiment**: a scientific procedure we do to discover something new, and test a hypothesis. For example, babies experiment by dropping objects to discover what happens to an object when it is dropped (and to test how their parents will react to those objects being dropped, over and over and over again ;) ).\n",
        "\n",
        "- **Analysis**: a detailed examination of something in order to understand it. For example, taking the results of an experiment tracking how objects fall, and analysing them by fitting a formula to these results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM6o-SLUrslH"
      },
      "source": [
        "And finally, after we have analysed our results we move on to..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z4vZELeG8Hw"
      },
      "source": [
        "**Step 3**: Validate your hypothesis based on the results.\n",
        "\n",
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/scientific-thinking/method_step_3.png\" alt=\"drawing\" height=\"350\"/\u003e\n",
        "\u003c/center\u003e\n",
        "\n",
        "\n",
        "The results can support or refute the hypothesis, but can also be inconclusive.\n",
        "\n",
        "If the results support the hypothesis, yay! Our hypothesis, now a working hypothesis, becomes an experimentally supported piece of knowledge that we can rely on and build on in the future.\n",
        "\n",
        "However, and we cannot stress this enough, refuted and inconclusive hypotheses are *equally important* because they too are novel pieces of knowledge which, other than adding to the body of knowledge, also influence the next hypotheses.\n",
        "In addition, validation does not end the first time we collect data and formulate the first hypothesis.\n",
        "Much of science is *actively* trying to find evidence that would refute the hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGvAkHbzsZTi"
      },
      "source": [
        "All of this might sound pretty simple but can actually be quite hard in practice. To understand why... let's play a simple game!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_3ESZM2HhjQ"
      },
      "source": [
        "## Confirmation bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u_XQh6DHTUI"
      },
      "source": [
        "\n",
        "Let's think about a sequence of 3 numbers. Let's imagine there is a rule in my head to determine the sequence of 3 numbers. Can you guess what the rule is if I just give you one example? \n",
        "\n",
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/scientific-thinking/confirmation_bias.png\" alt=\"drawing\" height=\"400\"/\u003e\n",
        "\u003c/center\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ud7GcfVwzHq"
      },
      "outputs": [],
      "source": [
        "#@title Try to come up with triplets that follow the rule!\n",
        "Number1 = 2  #@param {type:\"integer\"}\n",
        "Number2 = 2  #@param {type:\"integer\"}\n",
        "Number3 = 2  #@param {type:\"integer\"}\n",
        "# Write code to get the number and return yes or no\n",
        "\n",
        "def check(x, y, z):\n",
        "  return z == x + y\n",
        "\n",
        "if check(Number1, Number2, Number3):\n",
        "  print('Well done! They follow the rule!')\n",
        "else:\n",
        "  print(f'Oh no, this is not quite right! ({Number1},{Number2},{Number3}) does not follow the rule, try again!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJiBRN6x2khz"
      },
      "source": [
        "### Why is science hard? \n",
        "Because we can make many mistakes along the way :)\n",
        "\n",
        "One of the simplest ways to make a mistake is to *fall in love with your hypothesis*. It may sound strange, but after forming a hypothesis and working on it for a long time, we tend to cheer for it; we want to see it proven.\n",
        "\n",
        "...but science doesn't care about that, it only cares for the truth, and so should we."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6p-01WMJbdh"
      },
      "source": [
        "\"The great tragedy of Science – the slaying of a beautiful hypothesis by an ugly fact.\" — Thomas Huxley\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QprXPIoHjkH"
      },
      "source": [
        "## Many hypotheses and Occam's Razor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdYgvijhSvMW"
      },
      "source": [
        "As we saw in the pattern game, in many cases, multiple hypotheses fit well with the *observations* we collected with our *experiment*. Which one should we choose then?\n",
        "\n",
        "According to the **Occam's Razor** principle, when two theories explain the data equally well, the **simplest hypothesis** (in other words, the one that requires fewer assumptions) is most likely to be true. Let's see why with an example.\n",
        "\n",
        "\n",
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/scientific-thinking/occams_razor.png\" alt=\"drawing\" height=\"350\"/\u003e\n",
        "\u003c/center\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgSBvdMOT8R1"
      },
      "source": [
        "### Occam's Razor's example\n",
        "\n",
        "\n",
        "Let's suppose you found a coin on the sidewalk and would like to know how it got there. One hypothesis could be that it fell out of someone's pocket. But it could also be the case that a pigeon stole it from someone's hand and dropped it on the sidewalk. \n",
        "\n",
        "For each hypothesis we are making different **assumptions**:\n",
        "* **Hypothesis 1:**\n",
        "  * Someone  had a coin in their pocket.\n",
        "  * The coin fell out of it.\n",
        "* **Hypothesis 2:**\n",
        "  * Someone  had a coin in their hand.\n",
        "  * A pigeon was flying by.\n",
        "  * The pigeon stole the coin from the person's hand.\n",
        "  * The pigeon then dropped the coin on the sidewalk.\n",
        "\n",
        "Both hypotheses explain the fact that we found the coin on the sidewalk (**observation**) equally well, but Occam's Razor tells us that the first hypothesis is more likely to be what happened, because it relies on fewer assumptions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y1eyyi7k47U"
      },
      "source": [
        "# The story of a smart horse...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svyi1RZru3ZF"
      },
      "source": [
        "Many times, the actual explanation is so simple, we may even miss it! This is what happened in the famous case of 'Clever Hans' - a horse that convinced everyone that it had learned to do maths! \n",
        "Our story is set in Germany..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJOw1lGaH4xT"
      },
      "source": [
        "At the beginning of the 20th century people were fascinated by Charles Darwin's discoveries and began studying how intelligent animals were.\n",
        "\n",
        "**Herr Wilhelm von Osten** was a maths teacher and an amateur horse trainer. He **taught his horse Hans basic maths** such as addition, subtraction, product, division and fractions, but also how to read, spell and understand spoken German and how to keep track of dates and time. He would ask Hans questions such as: \"How much is 23 minus 11?\", and Hans would tap its hoof 12 times in response. He began traveling around Germany to show how smart *Clever Hans* was."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdAFgzdg0_iX"
      },
      "source": [
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/scientific-thinking/smart_horse.png\" alt=\"drawing\" height=\"400\"/\u003e\n",
        "\u003c/center\u003e\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqtED1EV2Rfp"
      },
      "source": [
        "The show was such a success that soon scientists became interested in trying to determine how clever *Clever Hans* really was. Among them, a psychologist called **Carl Stumpf**, was put in charge by the German board of education to form a commission to **study the horse**. In September 1904, the commission concluded that Hans' astonishing responses did not appear to be the result of a trick. \n",
        "\n",
        "**Oskar Pfungst** was then appointed to conduct a detailed evaluation of *Clever Hans*' abilities. He **devised several experiments** to verify Herr Wilhelm's claims. At first, he isolated the horse and the questioner from the attendants to make sure the horse was not given any clue from them. In this setting, *Clever Hans* was right on almost all the questions. He then had someone else ask the questions to the horse, which proved that Herr Wilhelm did not devise a secret code to tell the horse the right answer. Finally, he put blinders on the horse to prevent it from seeing the questioner after the question was asked. \n",
        "\n",
        "When the questioner was not in *Clever Hans*' sight, **the accuracy of the horse's answers went from 89% to only 6%**! Finally, Pfungst conducted another experiment where the questioner was in sight, but didn't know the answer to the question. Similarly, the horse's performance dropped.\n",
        "\n",
        "This demonstrated that the horse learned to slow down its taps and read cues from the people in front of him, to perceive when they were expecting it to stop tapping. Von Osten, as well as many other people, **genuinely thought the horse learned how to do calculus** and answer simple questions, when in reality Pfungst showed that he could get the horse to stop at any time just by raising his eyebrows slightly!\n",
        "\n",
        "So many people were convinced that Hans could do maths, they **never stopped to question if there was another explanation for his behavior**. In many ways Hans was quite clever to have fooled so many people!\n",
        "\n",
        "But once again, **Occam's Razor was correct**: the simplest explanation was the correct one!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VXHY-zN9r-J"
      },
      "source": [
        "## Occam's Razor probabilistic justification:\n",
        "\n",
        "This sounds somewhat intuitive, but can also be explained from a probabilistic point of view: in other words, using the tools of probability theory.\n",
        "\n",
        "\n",
        "The **probability** $P(A)$ of an event $A$ is, by definition, a real number between 0 (0%, *always false*) and 1 (100%, *always true*). The **joint probability** of some *independent* events, which measures the probability of all events occurring together, is the product of the probabilities of each event:\n",
        "\n",
        "$$\n",
        "P(A, B, C, \\dots) = P(A) \\cdot P(B) \\cdot P(C) \\cdot \\dots\n",
        "$$\n",
        "\n",
        "\u003e (It is important to note here that this formula applies only to independent (uncorrelated) events. The general formula is a bit more involved.)\n",
        "\n",
        "\n",
        "Great! But how can this help us understand Occam's razor? Let's ask Clever Hans!\n",
        "\n",
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/scientific-thinking/smart_prob_horse.png\" alt=\"drawing\" height=\"500\"/\u003e\n",
        "\u003c/center\u003e\n",
        "\n",
        "The **probability of an hypothesis** is equal to the **joint probability of the assumptions** that sustain that hypothesis. By the definition of probability that we defined earlier, an event has probability 1 if and only if it's certain. In all other cases its probability will be non-negative (greater or equal than zero), and **smaller than 1**. \n",
        "\n",
        "The product of two numbers smaller than $1$ is smaller than each of the numbers. **The product of the probabilities** of multiple uncertain assumptions will hence grow **smaller and smaller the more assumptions** we have. \n",
        "\n",
        "Wait, what? I'm not sure Clever Hans understood, can you be more clear? Of course, let's see an example with some numbers!\n",
        "\n",
        "### Example:\n",
        "Let's go back to our two hypotheses about the coin!\n",
        "\n",
        "The first hypothesis was based on two assumptions, while the second on four. Let's suppose the likelihood of the assumptions were as follows:\n",
        "\n",
        "* Hypothesis 1:\n",
        "  * P(A = Someone had a coin in their pocket) = 0.8\n",
        "  * P(B = The coin fell out of it) = 0.9\n",
        "\n",
        "* Hypothesis 2:\n",
        "  * P(C = Someone had a coin in their hand) = 0.9\n",
        "  * P(D = A pigeon was flying by) = 0.8\n",
        "  * P(E = The pigeon stole the coin from the person's hand) = 0.6\n",
        "  * P(F = The pigeon then dropped the coin on the sidewalk) = 0.9\n",
        "\n",
        "Remember: an event with probability 0.84 means that the event has 84% likelihood to happen.\n",
        "\n",
        "Now, let's compute the joint probability of the conditions of each hypothesis:\n",
        "\n",
        "* Hypothesis 1:\n",
        "  $$\n",
        "  P(A, B) = 0.9 \\cdot 0.8 = 0.72\\\\\n",
        "  $$\n",
        "\n",
        "* Hypothesis 2:\n",
        "  $$\n",
        "  P(C, D) = 0.9 \\cdot 0.8 = 0.72\\\\\n",
        "  P(C, D, E) = 0.72 \\cdot 0.6 = 0.432\\\\\n",
        "  P(C, D, E, F) = 0.432 \\cdot 0.9 = 0.3888\n",
        "  $$\n",
        "\n",
        "It is easy to see that the more (uncertain) assumptions an hypothesis has, the more unlikely it is for it to be correct. Just as predicted by the **Occam's Razor principle**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfYuP6GRJrer"
      },
      "source": [
        "# Science and Reinforcement Learning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pMAPb9NJres"
      },
      "source": [
        "We can also use Occam's razor in many other fields, such as in reinforcement learning, a topic that is of interest to many researchers at DeepMind. In reinforcement learning, the agent (for instance, the robot) is incentivised to do a particular task by being offered rewards for its behaviour. The agent then simply tries to get as much reward as it can. For example, giving a treat to your dog when it gives you its paw, when teaching it to \"shake your hand\", leads to your dog learning the skill, because the dog wants to get more treats.\n",
        "To maximise the reward, the agent needs to discover what is rewarding! So in many ways it is just like a scientist - trying things, observing what happens, learning from mistakes and repeating!\n",
        "\n",
        "\n",
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/scientific-thinking/rl.png\" width=\"600\"/\u003e\n",
        "\u003c/center\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zJ8jSIHw5gq"
      },
      "source": [
        "But this also means the agents we train will face the same hurdles that we do. As they explore the world to discover where the rewards are they must learn to use observations to guide their behavior! To understand this better, let us play a game where we are the agent trying to discover the reward in our world..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKMVUUjfJ0p7"
      },
      "source": [
        "# Let's play some games!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy5T4UJau_FI"
      },
      "source": [
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/scientific-thinking/where_rewards.png\" alt=\"drawing\" height=400\"/\u003e\n",
        "\u003c/center\u003e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWeEup1_vH9j"
      },
      "source": [
        "**HOW TO PLAY:**\n",
        "\n",
        "\n",
        "In the Cells below, whenever the game is in 'Experiment Mode', you can control the agent with your keyboard. To play the game:\n",
        "\n",
        "First - \n",
        "\n",
        "*   Make sure to run the cell where you want to play.\n",
        "\n",
        "*   Click on the game image to select it and type in **'w'** to **move up**; **'s'** to **move down**; **'a'** to **move to the left** and **'d'** to **move to the right**.\n",
        "\n",
        "*   When you hit **'Enter'** the **episode** (one round of the game) **ends**, the reward for that episode is printed onto the display and a new episode is started.\n",
        "\n",
        "\n",
        "\n",
        "The game may lag a little so be patient :) \n",
        "\n",
        "It's time to go exploring!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idyQkzFqPTp2"
      },
      "source": [
        "## Game 1: Can you guess the reward?!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlEpsowVxm8q"
      },
      "source": [
        "Lets start with a simple example. Play with the game below and try to figure out what the reward is..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXcfaM40KH8n"
      },
      "outputs": [],
      "source": [
        "#@title Game 1:  Experiment Mode.\n",
        "\n",
        "level = 'L1'\n",
        "game_type = 'experiment'\n",
        "\n",
        "game_keeper = GameKeeper(game_type, level)\n",
        "img = Img(game_keeper.obs, brighten_factor=2.5)\n",
        "output.register_callback('notebook.UpdateImg', game_keeper.move)\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        "      document.addEventListener(\"keydown\", async function(e){      \n",
        "      const result = await google.colab.kernel.invokeFunction(\n",
        "        'notebook.UpdateImg', // The callback name.\n",
        "        [e.key], // The arguments.\n",
        "        {}); // kwargs\n",
        "    });\n",
        "'''))\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zBS_bp6WBUs"
      },
      "source": [
        "Once you have an idea of what the reward is, in the map shown below, enter what you think the reward should be for the path taken in blue (when the path leads to an object, assume the agent went onto the object). Try to experiment a few times before looking at the solution in the cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYwYu2NsSphY"
      },
      "outputs": [],
      "source": [
        "#@title Game 1:  Evaluation Map\n",
        "\n",
        "level = 'L1'\n",
        "game_type = 'eval'\n",
        "\n",
        "game_keeper = GameKeeper(game_type, level)\n",
        "img_eval = Img(game_keeper.obs, brighten_factor=2.5)\n",
        "img_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixttSgedVY4n"
      },
      "outputs": [],
      "source": [
        "# @title Fill out answers\n",
        "reward = 0  #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "if reward == 3.0:\n",
        "  print(\"That's right! The agent will get a reward of 3.0!\")\n",
        "else:\n",
        "  print(\"Oh no! That wasn't right. Try again :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk6CoW9myGOU"
      },
      "source": [
        "What is the *maximum* reward any agent could get on the second map?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFOQWZFjWWWX"
      },
      "outputs": [],
      "source": [
        "maximum_possible_reward = 0  #@param {type:\"number\"}\n",
        "if maximum_possible_reward == 3.0:\n",
        "  print(\"That's right! The agent can get a maximum reward of 3.0!\")\n",
        "else:\n",
        "  print(\"Oh no! That wasn't the maximum reward :( What is the best thing the agent can do here?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZlon47pZ4ns"
      },
      "source": [
        "## Game 2: Can you figure out the reward rule?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo_UkCH0yNS5"
      },
      "source": [
        "Alright! Now that we're warmed up, lets look at another game. Remember.. this is a completely new game in a new world with its own rules!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWnp9GcjZ4nt"
      },
      "outputs": [],
      "source": [
        "#@title Game 2:  Experiment Mode.\n",
        "\n",
        "level = 'L2'\n",
        "game_type = 'experiment'\n",
        "\n",
        "game_keeper = GameKeeper(game_type, level, per_timestep_reward=-1.0)\n",
        "img = Img(game_keeper.obs, brighten_factor=2.5)\n",
        "output.register_callback('notebook.UpdateImg', game_keeper.move)\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        "      document.addEventListener(\"keydown\", async function(e){      \n",
        "      const result = await google.colab.kernel.invokeFunction(\n",
        "        'notebook.UpdateImg', // The callback name.\n",
        "        [e.key], // The arguments.\n",
        "        {}); // kwargs\n",
        "    });\n",
        "'''))\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqRvxEP5ydlP"
      },
      "source": [
        "Can you come up with multiple hypotheses to explain your observations? Are they falsifiable? Try to eliminate your hypothesis until only one is left!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3LnkfH1Z4nw"
      },
      "source": [
        "In the map shown below, enter what you think the reward should be for the path taken in blue (when the path leads to an object, assume the agent went onto the object). Try to experiment a few times before looking at the solution in the cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vQBIKVqZ4nx"
      },
      "outputs": [],
      "source": [
        "#@title Game 2:  Evaluation Map\n",
        "\n",
        "level = 'L2'\n",
        "game_type = 'eval'\n",
        "\n",
        "game_keeper = GameKeeper(game_type, level, per_timestep_reward=-1.0)\n",
        "img_eval = Img(game_keeper.obs, brighten_factor=2.5)\n",
        "img_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p92NcX09Z4n0"
      },
      "outputs": [],
      "source": [
        "# @title Fill out answers\n",
        "reward = 0  #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "if reward == -10.0:\n",
        "  print(\"That's right! The agent will get a reward of -10.!\")\n",
        "else:\n",
        "  print(\"Oh no! That wasn't right. Try again :)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz4MLXscZ4n3"
      },
      "outputs": [],
      "source": [
        "maximum_possible_reward = 0  #@param {type:\"number\"}\n",
        "if maximum_possible_reward == 0.0:\n",
        "  print(\"That's right! The agent can get a maximum reward of 0.0!\")\n",
        "else:\n",
        "  print(\"Oh no! That wasn't the maximum reward :( What is the best thing the agent can do here?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvHh0ZNkfO26"
      },
      "source": [
        "## Game 3: Can you figure out the reward rule?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khQ3XWDgyxmJ"
      },
      "source": [
        "OK! Lets do one more map. Remember - each new map is a whole new world with its own rules. All we have is our science toolbox to help us along the way..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDgxK0dvfO28"
      },
      "outputs": [],
      "source": [
        "#@title Game 3:  Experiment Mode.\n",
        "\n",
        "level = 'L3'\n",
        "game_type = 'experiment'\n",
        "\n",
        "game_keeper = GameKeeper(game_type, level)\n",
        "img = Img(game_keeper.obs, brighten_factor=2.5)\n",
        "output.register_callback('notebook.UpdateImg', game_keeper.move)\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        "      document.addEventListener(\"keydown\", async function(e){      \n",
        "      const result = await google.colab.kernel.invokeFunction(\n",
        "        'notebook.UpdateImg', // The callback name.\n",
        "        [e.key], // The arguments.\n",
        "        {}); // kwargs\n",
        "    });\n",
        "'''))\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZYxME1ly_fq"
      },
      "source": [
        "Sometimes it helps to write down our hypothesis and what assumptions they each make. When we write things down explicitly, Occam's razor can often help us figure out what is most likely to be true!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "451NV-bgfO3C"
      },
      "source": [
        "In the map shown below, enter what you think the reward should be for the path taken in blue (when the path leads to an object, assume the agent went onto the object). Try to experiment a few times before looking at the solution in the cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIZoCBQwfO3C"
      },
      "outputs": [],
      "source": [
        "#@title Game 3:  Evaluation Map\n",
        "\n",
        "level = 'L3'\n",
        "game_type = 'eval'\n",
        "\n",
        "game_keeper = GameKeeper(game_type, level)\n",
        "img_eval = Img(game_keeper.obs, brighten_factor=2.5)\n",
        "img_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebr63h1pfO3E"
      },
      "outputs": [],
      "source": [
        "# @title What reward would the agent get?\n",
        "reward = 0  #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "if reward == 0.0:\n",
        "  print(\"That's right! The agent will get a reward of 0.0!\")\n",
        "else:\n",
        "  print(\"Oh no! That wasn't right. Try again :)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkZ7fCjzfO3H"
      },
      "outputs": [],
      "source": [
        "maximum_possible_reward = 0  #@param {type:\"number\"}\n",
        "if maximum_possible_reward == 10.0:\n",
        "  print(\"That's right! The agent can get a maximum reward of 10.0!\")\n",
        "else:\n",
        "  print(\"Oh no! That wasn't the maximum reward :( What is the best thing the agent can do here?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//research/colab/notebook:notebook_backend_py3",
        "kind": "private"
      },
      "name": "Scientific Thinking",
      "provenance": [
        {
          "file_id": "1fOHpIbcWhQlDblhqSSCONmoF8_Av_Nlm",
          "timestamp": 1594806037576
        },
        {
          "file_id": "1R8GxdywKO93UKLZ4lWuOqhNi2LEqCCis",
          "timestamp": 1594390708275
        },
        {
          "file_id": "1UXIAOVY6aA_ND6URVaDlmAikxT0W-2oT",
          "timestamp": 1592998541423
        },
        {
          "file_id": "15ojzBQDWibEER4MPL7_w0MLSZqx7CrJB",
          "timestamp": 1592836943352
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
