{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Some_EDA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWM3oK2s/V79wuv+I2ZmNP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aboubacar2012/ML_Project/blob/main/Some_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5H3mLOdcmM7"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlax7GpUcx2F"
      },
      "source": [
        "# Finf the median price by the interest level\n",
        "prices=df.groupby('interest_level', as_index=False)['price'].median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4dwaeZ0dF-0"
      },
      "source": [
        "# Draw a barplot \n",
        "fig=plt.figure(figsize=(7,5))\n",
        "plt.bar(prices.interest_level, prices.price, width=0.5, alpha=0.8)\n",
        "# Set titles \n",
        "plt.xlabel('Interest level')\n",
        "plt.ylabel('Median price')\n",
        "plt.title('Median listing price across interest level')\n",
        "show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTzODAtWeykS"
      },
      "source": [
        "# Calculate the ride distance\n",
        "train['distance_km'] = haversine_distance(train)\n",
        "\n",
        "# Draw a scatterplot\n",
        "plt.scatter(x=train['fare_amount'], y=train['distance_km'], alpha=0.5)\n",
        "plt.xlabel('Fare amount')\n",
        "plt.ylabel('Distance, km')\n",
        "plt.title('Fare amount based on the distance')\n",
        "\n",
        "# Limit on the distance\n",
        "plt.ylim(0, 50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee0MALb-hY8X"
      },
      "source": [
        "# Create hour feature\n",
        "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n",
        "train['hour'] = train.pickup_datetime.dt.hour\n",
        "\n",
        "# Find median fare_amount for each hour\n",
        "hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()\n",
        "\n",
        "# Plot the line plot\n",
        "plt.plot(hour_price['hour'], hour_price['fare_amount'], marker='o')\n",
        "plt.xlabel('Hour of the day')\n",
        "plt.ylabel('Median fare amount')\n",
        "plt.title('Fare amount based on day time')\n",
        "plt.xticks(range(24))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHTUAhK4k1Lo"
      },
      "source": [
        "# Local validation \n",
        "#Import KFold \n",
        "from sklearn.model_selection import KFold\n",
        "#Create a KFold object \n",
        "kf=KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "#Loop through each cross-validation split \n",
        "for train_index, test_index in kf.split(train):\n",
        "  #Get training and testing data for the corresponding split \n",
        "  cv_train, cv_test=train.iloc[train_index], train.iloc[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9aI5jmpl3-P"
      },
      "source": [
        "# Stratified K-fold\n",
        "# Import StratifiedKfold \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "#Create a stratifiedKFold object\n",
        "str_kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "# Loop through each cross-validation split\n",
        "for train_index, test_index in str_kf.split(train, train['target']):\n",
        "  cv_train, cv_test=train.iloc[train_index], train.iloc[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DZCRYrmm5in"
      },
      "source": [
        "# K-fold cross-validation\n",
        "You will start by getting hands-on experience in the most commonly used K-fold cross-validation.\n",
        "\n",
        "The data you'll be working with is from the \"Two sigma connect: rental listing inquiries\" Kaggle competition. The competition problem is a multi-class classification of the rental listings into 3 classes: low interest, medium interest and high interest. For faster performance, you will work with a subsample consisting of 1,000 observations.\n",
        "\n",
        "You need to implement a K-fold validation strategy and look at the sizes of each fold obtained. train DataFrame is already available in your workspace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1TAS6vSnDFH"
      },
      "source": [
        "-Create a KFold object with 3 folds.\n",
        "-Loop over each split using the kf object.\n",
        "-For each split select training and testing folds using train_index and test_index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2_egHHom_3n"
      },
      "source": [
        "# Import KFold\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Create a KFold object\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
        "\n",
        "# Loop through each split\n",
        "fold = 0\n",
        "for train_index, test_index in kf.split(train):\n",
        "    # Obtain training and testing folds\n",
        "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "    print('Fold: {}'.format(fold))\n",
        "    print('CV train shape: {}'.format(cv_train.shape))\n",
        "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6POrRzNrijZ"
      },
      "source": [
        "Import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mutgn58LoMI9"
      },
      "source": [
        "# Import StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Create a StratifiedKFold object\n",
        "str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
        "\n",
        "# Loop through each split\n",
        "fold = 0\n",
        "for train_index, test_index in str_kf.split(train, train['interest_level']):\n",
        "    # Obtain training and testing folds\n",
        "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "    print('Fold: {}'.format(fold))\n",
        "    print('CV train shape: {}'.format(cv_train.shape))\n",
        "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmIAxXvbrewc"
      },
      "source": [
        "Time K-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv_O912coruy"
      },
      "source": [
        "# Time K-fold cross-validation \n",
        "# Import TimeSeriesSplit\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "#Create a TimeSeriesSplit object\n",
        "time_kfold=TimeSeriesSplit(n_splits=5)\n",
        "#Sort train by date \n",
        "train=train.sort_values('date')\n",
        "#Loop through each cross-validation split \n",
        "for train_index, test_index in time_kfold.split(train):\n",
        "  cv_train, cv_test=train.iloc[train_index], train.iloc[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R2hp9yYrmkC"
      },
      "source": [
        "Validation pipeline "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CitOyNiQrbwf"
      },
      "source": [
        "# list for results \n",
        "fold_metrics=[]\n",
        "for train_index, test_index in CV_STRATEGY.split(train):\n",
        "  cv_train, cv_test=train.iloc[train_index], train.iloc[test_index]\n",
        "  #Train a model \n",
        "  model.fit(cv_train)\n",
        "  #Make predictions\n",
        "  predictions=model.predict(cv_test)\n",
        "  #Calculate the metrics\n",
        "  metric=evaluate(cv_test, predictions)\n",
        "  fold_metrics.append(metric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQca4WR4sbVl"
      },
      "source": [
        "Overall validation score "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3jJ8rAsseVa"
      },
      "source": [
        "import numpy as np \n",
        "# Simple mean over the folds \n",
        "mean_score=np.mean(fold_metrics)\n",
        "# Overall validation scoe \n",
        "overall_score_minimizing=np.mean(fold_metrics)+np.std(fold_metrics)\n",
        "# Or \n",
        "overall_score_minimizing=np.mean(fold_metrics)-np.std(fold_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuW4qZ-IUSlr"
      },
      "source": [
        "-- Features Engineering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbsWcQvuUVEg"
      },
      "source": [
        "# Creating features \n",
        "# Concatenate the train and test data \n",
        "data=pd.concat([train, test])\n",
        "# Create new features for the data frame \n",
        "# Get the trian and test back \n",
        "train=data[data.id.isin(train.id)]\n",
        "test=data[data.id.isin(test.id)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-JUbizoViJE"
      },
      "source": [
        "---- Date time features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUivlUecVhGz"
      },
      "source": [
        "df.head()\n",
        "#Convert date to the datetime object\n",
        "df['date']=pd.to_datetime(df['date'])\n",
        "#Year feature \n",
        "df['Year']=df['date'].dt.year\n",
        "#Month features \n",
        "df['month']=df['date'].dt.month\n",
        "# Week features\n",
        "df['week']=df['date'].dt.weekofyear \n",
        "#Day features \n",
        "df['dayofyear']=df['date'].dt.dayofyear\n",
        "df['dayofmonth']=df['date'].dt.day\n",
        "df['dayofweek']=df['date'].dt.dayofweek "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FS2KmE4X_mR"
      },
      "source": [
        "---- Categorical Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQgnQmeFX9ti"
      },
      "source": [
        "# Label encoding \n",
        "\n",
        "#Import LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "# ©reate a LabelEncoder object\n",
        "le=LabelEncoder()\n",
        "#Encode a categorical feature \n",
        "df['cat_encoded']=le.fit_transform(df['cat'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S9fW-1faGeX"
      },
      "source": [
        "----One-Hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSRgyhiRaLly"
      },
      "source": [
        "# To handle ranking depend let's use One-Hot encoding \n",
        "# One-Hot encoding\n",
        "# Create One-Hot encoded feature \n",
        "ohe=pd.get_dummies(df['cat'], prefix='ohe_cat')\n",
        "#Drop the initial feature \n",
        "df.drop('cat', axis=1, inplace=True)\n",
        "#Concatenate OHE features to the dataframe \n",
        "df=pd.concat([df, ohe], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvv0fLPWaYl3"
      },
      "source": [
        "--- Binary Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAlQJhKmaaZy"
      },
      "source": [
        "# Dataframe with a features (yes, no)\n",
        "le=LabelEncoder()\n",
        "df['binary_encoded']=le.fit_transform(df['binary_features'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNr6U7HBb13o"
      },
      "source": [
        "--- Target encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN_lbIDbb3-l"
      },
      "source": [
        "def mean_target_encoding(train, test, target, categorical, alpha=5):\n",
        "  \n",
        "    # Get the train feature\n",
        "    train_feature = train_mean_target_encoding(train, target, categorical, alpha)\n",
        "  \n",
        "    # Get the test feature\n",
        "    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)\n",
        "    \n",
        "    # Return new features to add to the model\n",
        "    return train_feature, test_feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eocJjaa3fGO1"
      },
      "source": [
        "# Create mean target encoded feature\n",
        "train['RoofStyle_enc'], test['RoofStyle_enc'] = mean_target_encoding(train=train,\n",
        "                                                                     test=test,\n",
        "                                                                     target='SalePrice',\n",
        "                                                                     categorical='RoofStyle',\n",
        "                                                                     alpha=10)\n",
        "\n",
        "# Look at the encoding\n",
        "print(test[['RoofStyle', 'RoofStyle_enc']].drop_duplicates())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at-dUPrOfLWW"
      },
      "source": [
        "---- Missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfmGyLnPfM2C"
      },
      "source": [
        "# Imputation strategies of missing data\n",
        "# Numerical data\n",
        "# - Mean/median imputation \n",
        "# - fillna (-999) not suitable for linear model\n",
        "\n",
        "# Categorical data\n",
        "# - Most frequent category imputation \n",
        "# - New category imputation \n",
        "\n",
        "#Numerical missing data \n",
        "# Import SimpleImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "# Different type of imputers\n",
        "mean_imputer=SimpleImputer(strategy='mean')\n",
        "constant_imputer=SimpleImputer(strategy='constant', fill_value=-999)\n",
        "#Imputation\n",
        "df[['num']=mean_imputer.fit_transform(df[['num']])]\n",
        "\n",
        "#Imupation of categorical data \n",
        "from sklearn.imput import SimpleImputer\n",
        "\n",
        "#Different type of imputers\n",
        "frequent_imputer=SimpleImputer(strategy='most_frequent')\n",
        "constant_imputer=SimpleImputer(strategy='constant', fill_value='MISS')\n",
        "# Imputation\n",
        "df[['cat']]=constant_imupter.fit_transform(df[['cat']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enabicDfke29"
      },
      "source": [
        "---- Baseline model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWOmpFAekgrA"
      },
      "source": [
        "# Exple : New york city taxi validation \n",
        "# Read data \n",
        "taxi_train=pd.read_csv('taxi_train.csv')\n",
        "tax_test=pd.read_csv('taxi_test.csv')\n",
        "\n",
        "# Hould-out splittting \n",
        "from sklearn.model_selection import train_test_split\n",
        "#Create local validation \n",
        "validation_train, validation_test=train_test_split(taxi_train, test_size=0.3\n",
        "                                                   random_state=123)\n",
        "\n",
        "#Baseline model 1 \n",
        "\n",
        "#Assign the mean fare amount to all the test observations \n",
        "taxi_test['fare_amount']=np.mean(taxi_train.fare_amount)\n",
        "# Write predictions to the file \n",
        "taxi_test[['id','fare_amount']].to_csv('mean_sub.csv', index=False)\n",
        "#output : validation RMSE=9.986, Public LB RMSE=9.409, public LB Position=1449/1500\n",
        "\n",
        "# Baseline model II\n",
        "\n",
        "#Calculate the mean fare amount by group \n",
        "naive_prediction_groups=taxi_train.groupby('passenger_count').fare_amount.mean()\n",
        "# Make predictions on the test set \n",
        "taxi_test['fare_amount']=taxi_test.passenger_count.map(naive_prediction_groups)\n",
        "#Write predictions to the file \n",
        "taxi_test[['id','fare_amount']].to_csv('mean_group_sub.csv', index=False)\n",
        "# output : validation RMSE=9.978, Public LB RMSE=9.409, Public LB Position=1411/1500\n",
        "\n",
        "# Baseline model III\n",
        "features=['pickup_longitude','pickup_latitude',\n",
        "          'dropoff_longititude','dropoff_latitude','passenger_count']\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor \n",
        "#Train a Gradient Boosting model \n",
        "gb=GradientBoostingRegressor()\n",
        "gb.fit(taxi_train[feature], taxi_train.fare_amount)\n",
        "#Make predictions on the test data \n",
        "taxi_test['fare_amount']=gb.predict(taxi_test[features])\n",
        "#Write predictions to the file \n",
        "taxi_test[['id','fare_amount']].to_csv('gb_sub.csv', index=False)\n",
        "#ouput : Validaton RMSE=5.996, public LB RMSE=4.595, Public LB position=1109/1500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNkswy5c4L_p"
      },
      "source": [
        "----Hyperpameter tunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3uyAq174RGE"
      },
      "source": [
        "# Grid Search \n",
        "\n",
        "# Possible alpha values \n",
        "alpha_grid=[0.01,0.1, 1, 10]\n",
        "from sklearn.linear_model import Ridge\n",
        "results={}\n",
        "# For each values in the grid \n",
        "for candidate_alpha in alpha_grid : \n",
        "  # Create a model with a specific alpha value \n",
        "  ridge_regression=Ridget(alpha=candidate_alpha)\n",
        "  # Find the validation score for this model\n",
        "  # Save the results for each alpha values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhd0Ho3j7DDx"
      },
      "source": [
        "import itertools\n",
        "\n",
        "# Hyperparameter grids\n",
        "max_depth_grid = [3, 5, 7]\n",
        "subsample_grid = [0.8, 0.9, 1.0]\n",
        "results = {}\n",
        "\n",
        "# For each couple in the grid\n",
        "for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid):\n",
        "    params = {'max_depth': max_depth_candidate,\n",
        "              'subsample': subsample_candidate}\n",
        "    validation_score = get_cv_score(train, params)\n",
        "    # Save the results for each couple\n",
        "    results[(max_depth_candidate, subsample_candidate)] = validation_score   \n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSB_VWPE8uAC"
      },
      "source": [
        "--- Model ensembling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4o8uZ2B8wCj"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "\n",
        "# Train a Gradient Boosting model\n",
        "gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf = RandomForestRegressor().fit(train[features], train.fare_amount)\n",
        "\n",
        "# Make predictions on the test data\n",
        "test['gb_pred'] = gb.predict(test[features])\n",
        "test['rf_pred'] = rf.predict(test[features])\n",
        "\n",
        "# Find mean of model predictions\n",
        "test['blend'] = (test['gb_pred'] + test['rf_pred']) / 2\n",
        "print(test[['gb_pred', 'rf_pred', 'blend']].head(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXjnS321Aq-7"
      },
      "source": [
        "--- Model stacking I"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzPkNER7ApBY"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "\n",
        "# Split train data into two parts\n",
        "part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n",
        "\n",
        "# Train a Gradient Boosting model\n",
        "gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n",
        "\n",
        "# Train a Random Forest model on Part 1\n",
        "rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}